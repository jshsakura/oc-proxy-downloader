# -*- coding: utf-8 -*-
import sys
import os
import locale
import logging
import signal
import atexit

# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì •
if sys.platform.startswith('win'):
    try:
        # Windowsì—ì„œ UTF-8 ê°•ì œ ì„¤ì •
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        pass

# ê°•ì œ stdout ì¶œë ¥ ì„¤ì •
def force_print(*args, **kwargs):
    message = ' '.join(str(arg) for arg in args)
    sys.stdout.write(f"{message}\n")
    sys.stdout.flush()

# ë¡œê¹… ë ˆë²¨ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
LOG_LEVEL = os.getenv('LOG_LEVEL', 'DEBUG').upper()  # ê¸°ë³¸ê°’ì„ WARNINGìœ¼ë¡œ ë³€ê²½

# ë¡œê·¸ì¸ ì¸ì¦ ì„¤ì • (í™˜ê²½ë³€ìˆ˜)
AUTH_USERNAME = os.getenv('AUTH_USERNAME')  # ë¡œê·¸ì¸ ì‚¬ìš©ìëª…
AUTH_PASSWORD = os.getenv('AUTH_PASSWORD')  # ë¡œê·¸ì¸ ë¹„ë°€ë²ˆí˜¸
JWT_SECRET_KEY = os.getenv('JWT_SECRET_KEY', 'default-secret-key-change-in-production')  # JWT ì‹œí¬ë¦¿
JWT_ALGORITHM = os.getenv('JWT_ALGORITHM', 'HS256')
JWT_EXPIRATION_HOURS = int(os.getenv('JWT_EXPIRATION_HOURS', '24'))  # í† í° ë§Œë£Œ ì‹œê°„

# ì¸ì¦ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
AUTHENTICATION_ENABLED = bool(AUTH_USERNAME and AUTH_PASSWORD)

# ë¡œê·¸ ë ˆë²¨ì— ë”°ë¥¸ ë©”ì‹œì§€ í•„í„°ë§
def smart_print(*args, **kwargs):
    message = ' '.join(str(arg) for arg in args)
    
    # LOG_LEVELì— ë”°ë¥¸ í•„í„°ë§
    if LOG_LEVEL == 'ERROR' and not any(tag in message for tag in ['[ERROR]']):
        return
    elif LOG_LEVEL == 'WARNING' and not any(tag in message for tag in ['[ERROR]', '[WARNING]']):
        return
    elif LOG_LEVEL == 'INFO' and not any(tag in message for tag in ['[ERROR]', '[WARNING]', '[LOG]']):
        return
    # DEBUG ë ˆë²¨ì´ë©´ ëª¨ë“  ë©”ì‹œì§€ ì¶œë ¥
    
    sys.stdout.write(f"{message}\n")
    sys.stdout.flush()

# ê¸°ì¡´ printë¥¼ smart_printë¡œ ëŒ€ì²´
import builtins
original_print = builtins.print
builtins.print = smart_print

# Python ê²½ë¡œ ì„¤ì • (Docker í™˜ê²½ ëŒ€ì‘)
backend_path = os.path.dirname(os.path.abspath(__file__))
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

from core.config import get_config, save_config, get_download_path
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, Depends, BackgroundTasks, HTTPException, Body, APIRouter
from contextlib import asynccontextmanager
from websockets.exceptions import ConnectionClosedError, ConnectionClosedOK
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from sqlalchemy.orm import Session
from pydantic import BaseModel, HttpUrl
from core.models import Base, DownloadRequest, StatusEnum, ProxyStatus
from core.db import engine, get_db
from core.common import convert_size
from core.proxy_manager import get_unused_proxies, get_user_proxy_list, mark_proxy_used, reset_proxy_usage, test_proxy
import requests
import os
import datetime
import random
import lxml.html
import time
import asyncio
from core.i18n import get_message, load_all_translations, get_translations
from sqlalchemy import or_, desc, asc
import queue
import cloudscraper
import re
from urllib.parse import urlparse, unquote
import os
from pathlib import Path
try:
    import tkinter as tk
    from tkinter import filedialog
    GUI_AVAILABLE = True
except ImportError:
    GUI_AVAILABLE = False
import threading
from concurrent.futures import ThreadPoolExecutor, Future
import weakref
import multiprocessing
from typing import Optional
from core.downloader import router as downloader_router
from core.proxy_stats import router as proxy_stats_router
from core.auth_routes import router as auth_router

# Get the absolute path to the frontend/dist directory
backend_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(backend_dir)

# Docker í™˜ê²½ì—ì„œëŠ” static ë””ë ‰í† ë¦¬ ì‚¬ìš©, ê°œë°œ í™˜ê²½ì—ì„œëŠ” frontend/dist ì‚¬ìš©
if os.path.exists(os.path.join(backend_dir, "static")):
    frontend_dist_path = os.path.join(backend_dir, "static")
else:
    frontend_dist_path = os.path.join(project_root, "frontend", "dist")

# ë‹¤ìš´ë¡œë“œ ìŠ¤ë ˆë“œ ê´€ë¦¬
# ê³µìœ  ê°ì²´ë“¤ì„ ë³„ë„ ëª¨ë“ˆì—ì„œ import
from core.shared import download_manager, status_queue
from logger import log_once

# ê³µìœ  ê°ì²´ import ì™„ë£Œ


# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” (ìŠ¤í‚¤ë§ˆ ì—ëŸ¬ ì‹œ ìë™ ì¬ìƒì„±)
try:
    Base.metadata.create_all(bind=engine)
except Exception as e:
    print(f"[LOG] ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì—ëŸ¬ ê°ì§€: {e}")
    
    # SQLite íŒŒì¼ ì‚­ì œ - CONFIG_DIR ê²½ë¡œ ì‚¬ìš©
    from core.db import DB_PATH
    if DB_PATH.exists():
        os.remove(DB_PATH)
        print(f"[LOG] ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì‚­ì œ: {DB_PATH}")
    
    # ìƒˆë¡œìš´ ì—”ì§„ê³¼ ì„¸ì…˜ ìƒì„±
    from core.db import engine
    Base.metadata.create_all(bind=engine)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
_startup_executed = False

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    global _startup_executed
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¬í™•ì¸ ë° ì¬ìƒì„± (í•„ìš”ì‹œ)
    try:
        # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤ì´ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        db = next(get_db())
        test_query = db.query(DownloadRequest).filter(DownloadRequest.id == -1).first()
        db.close()
    except Exception as e:
        if "no such column" in str(e):
            print(f"[LOG] lifespanì—ì„œ ìŠ¤í‚¤ë§ˆ ì—ëŸ¬ ì¬ê°ì§€: {e}")
            print("[LOG] ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„± ì¤‘...")
            
            # SQLite íŒŒì¼ ì‚­ì œ - CONFIG_DIR ê²½ë¡œ ì‚¬ìš©
            from core.db import DB_PATH
            if DB_PATH.exists():
                os.remove(DB_PATH)
                print(f"[LOG] lifespanì—ì„œ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì‚­ì œ: {DB_PATH}")
            
            Base.metadata.create_all(bind=engine)
            print("[LOG] lifespanì—ì„œ ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        else:
            raise e
    
    # ì´ë¯¸ ì‹¤í–‰ë˜ì—ˆìœ¼ë©´ ë¬´ì¡°ê±´ ì¢…ë£Œ
    if not _startup_executed:
        _startup_executed = True
        
        # ë²ˆì—­ íŒŒì¼ ë¡œë“œ
        load_all_translations()
        
        # WebSocket broadcaster ì‹œì‘
        asyncio.create_task(status_broadcaster())
        
        # ì„œë²„ ì¬ì‹œì‘ ì‹œ ì§„í–‰ ì¤‘ì´ë˜ ë‹¤ìš´ë¡œë“œë¥¼ ëª¨ë‘ stoppedë¡œ ë³€ê²½ (ë³µêµ¬)
        print("[LOG] ğŸš€ ì„œë²„ ì‹œì‘ - ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ë³µêµ¬ ì‹œì‘")
        db = next(get_db())
        
        # ì§„í–‰ ì¤‘ì´ë˜ ë‹¤ìš´ë¡œë“œë“¤ì„ ê°€ì ¸ì™€ì„œ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        downloading_requests = db.query(DownloadRequest).filter(
            DownloadRequest.status.in_([StatusEnum.downloading, StatusEnum.proxying, StatusEnum.parsing])
        ).all()
        
        print(f"[LOG] ğŸ“Š ë³µêµ¬ ëŒ€ìƒ ë‹¤ìš´ë¡œë“œ: {len(downloading_requests)}ê°œ")
        
        for req in downloading_requests:
            print(f"[LOG] ğŸ”„ ë³µêµ¬ ì¤‘: ID {req.id} - {req.status} â†’ stopped")
            req.status = StatusEnum.stopped
            req.direct_link = None  # ì„œë²„ ì¬ì‹œì‘ ì‹œ íŒŒì‹± ìƒíƒœ ì´ˆê¸°í™”
            db.commit()
            
            # ê° ë‹¤ìš´ë¡œë“œì˜ ìƒíƒœ ë³€ê²½ì„ WebSocketìœ¼ë¡œ ì•Œë¦¼
            try:
                import json
                status_data = {
                    "type": "status_update",
                    "data": {
                        "id": req.id,
                        "status": "stopped",
                        "url": req.url,
                        "file_name": req.file_name,
                        "total_size": req.total_size,
                        "downloaded_size": req.downloaded_size,
                        "requested_at": req.requested_at.isoformat() if req.requested_at else None,
                        "direct_link": req.direct_link,
                        "use_proxy": req.use_proxy,
                        "error": "ì„œë²„ ì¬ì‹œì‘ìœ¼ë¡œ ì¸í•œ ì •ì§€"
                    }
                }
                status_queue.put(json.dumps(status_data))
            except Exception as e:
                print(f"[LOG] ì„œë²„ ì‹œì‘ ì‹œ WebSocket ì•Œë¦¼ ì‹¤íŒ¨: {e}")
        
        if len(downloading_requests) > 0:
            print(f"[LOG] âœ… ë³µêµ¬ ì™„ë£Œ: {len(downloading_requests)}ê°œ ë‹¤ìš´ë¡œë“œë¥¼ stoppedë¡œ ë³€ê²½")
        else:
            print("[LOG] âœ… ë³µêµ¬ ì™„ë£Œ: ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì—†ìŒ")
        
        # ì„œë²„ ì‹œì‘ ì‹œ pending ìƒíƒœì¸ ë‹¤ìš´ë¡œë“œë“¤ì„ ìë™ìœ¼ë¡œ ì‹œì‘
        pending_requests = db.query(DownloadRequest).filter(
            DownloadRequest.status == StatusEnum.pending
        ).all()
        
        if len(pending_requests) > 0:
            print(f"[LOG] ì„œë²„ ì‹œì‘ ì‹œ {len(pending_requests)}ê°œì˜ ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œ ë°œê²¬")
            db.close()
            
            # ì ì‹œ ëŒ€ê¸° í›„ ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œë“¤ ì‹œì‘ (ì„œë²„ ì™„ì „ ì´ˆê¸°í™” ëŒ€ê¸°)
            def start_pending_downloads():
                import time
                time.sleep(2)  # ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ ëŒ€ê¸°
                try:
                    from core.shared import download_manager
                    download_manager.check_and_start_waiting_downloads()
                    print(f"[LOG] ì„œë²„ ì‹œì‘ ì‹œ ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì™„ë£Œ")
                except Exception as e:
                    print(f"[LOG] ì„œë²„ ì‹œì‘ ì‹œ ëŒ€ê¸° ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì‹¤íŒ¨: {e}")
            
            import threading
            threading.Thread(target=start_pending_downloads, daemon=True).start()
        else:
            db.close()
            print(f"[LOG] ì„œë²„ ì‹œì‘ ì‹œ ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œ ì—†ìŒ")
        
    yield
    
    # Shutdown
    cleanup_and_exit()

app = FastAPI(lifespan=lifespan)
# print(f"[DEBUG] FastAPI ì•± ìƒì„±ë¨ - ID: {id(app)} PID: {os.getpid()}")

# ëª¨ë“  ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def log_requests(request, call_next):
    print(f"[API] === INCOMING REQUEST ===")
    print(f"[API] Method: {request.method}")
    print(f"[API] Path: {request.url.path}")
    print(f"[API] Query: {request.url.query}")
    if request.method == "POST":
        print(f"[API] Headers: {dict(request.headers)}")
    print(f"[API] ========================")
    
    response = await call_next(request)
    
    print(f"[API] === RESPONSE ===")
    print(f"[API] Status: {response.status_code}")
    print(f"[API] =================")
    
    return response

api_router = APIRouter(prefix="/api")
api_router.include_router(downloader_router)
api_router.include_router(auth_router)  # ì¸ì¦ ë¼ìš°í„° ì¶”ê°€

# WebSocket ì—°ê²° ê´€ë¦¬
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []
        self.max_connections = int(os.getenv('MAX_WEBSOCKET_CONNECTIONS', '50'))  # ì œí•œì„ 50ê°œë¡œ ëŠ˜ë¦¼
        self.connection_count = 0

    async def cleanup_dead_connections(self):
        """ì£½ì€ ì—°ê²°ë“¤ì„ ì •ë¦¬"""
        dead_connections = []
        for connection in self.active_connections:
            try:
                # ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ping ì‹œë„
                await connection.ping()
            except:
                # pingì´ ì‹¤íŒ¨í•˜ë©´ ì£½ì€ ì—°ê²°
                dead_connections.append(connection)
        
        for connection in dead_connections:
            self.disconnect(connection)
        
        if dead_connections:
            print(f"[LOG] {len(dead_connections)}ê°œì˜ ì£½ì€ WebSocket ì—°ê²° ì •ë¦¬ë¨")

    async def connect(self, websocket: WebSocket):
        # ë¨¼ì € ì£½ì€ ì—°ê²°ë“¤ ì •ë¦¬
        await self.cleanup_dead_connections()
        
        # ì—°ê²° ìˆ˜ ì œí•œ í™•ì¸ (ì—¬ì „íˆ ì´ˆê³¼í•˜ëŠ” ê²½ìš°)
        if len(self.active_connections) >= self.max_connections:
            print(f"[WARNING] WebSocket ì—°ê²° ìˆ˜ ì œí•œ ë„ë‹¬: {self.max_connections}ê°œ")
            await websocket.close(code=1008, reason="Too many connections")
            return False
        
        await websocket.accept()
        self.active_connections.append(websocket)
        self.connection_count += 1
        
        print(f"[LOG] WebSocket ì—°ê²°ë¨: {len(self.active_connections)}ê°œ")
        return True

    def disconnect(self, websocket: WebSocket):
        try:
            self.active_connections.remove(websocket)
            print(f"[LOG] WebSocket ì—°ê²° í•´ì œë¨: {len(self.active_connections)}ê°œ ë‚¨ìŒ")
        except ValueError:
            # ì´ë¯¸ ì œê±°ëœ ê²½ìš° ë¬´ì‹œ
            pass

    async def broadcast(self, message: str):
        if not self.active_connections:
            return
        
        disconnected = []
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except ConnectionClosedError:
                # ì—°ê²°ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë¨
                disconnected.append(connection)
            except ConnectionClosedOK:
                # ì—°ê²°ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë¨
                disconnected.append(connection)
            except Exception as e:
                # ê¸°íƒ€ ì˜ˆì™¸
                print(f"[LOG] WebSocket broadcast error: {e}")
                disconnected.append(connection)
        
        # ëŠì–´ì§„ ì—°ê²° ì œê±°
        for connection in disconnected:
            try:
                self.active_connections.remove(connection)
            except ValueError:
                # ì´ë¯¸ ì œê±°ëœ ê²½ìš° ë¬´ì‹œ
                pass

manager = ConnectionManager()

# status_queueëŠ” shared.pyì—ì„œ importë¨

async def status_broadcaster():
    print("[LOG] status_broadcaster ì‹œì‘ë¨")
    while True:
        try:
            # non-blockingìœ¼ë¡œ íì—ì„œ ë©”ì‹œì§€ í™•ì¸
            try:
                msg = status_queue.get_nowait()
                # print(f"[LOG] status_broadcaster: ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘, ì—°ê²° ìˆ˜: {len(manager.active_connections)}")  # ë„ˆë¬´ ë§ì€ ë¡œê·¸
                await manager.broadcast(msg)
                # print(f"[LOG] status_broadcaster: ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì™„ë£Œ")  # ë„ˆë¬´ ë§ì€ ë¡œê·¸
            except queue.Empty:
                # íê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¢€ ë” ì˜¤ë˜ ëŒ€ê¸° (CPU ì‚¬ìš©ëŸ‰ ìµœì í™”)
                await asyncio.sleep(0.5)
        except asyncio.CancelledError:
            # ì„œë²„ ì¢…ë£Œ ì‹œ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œ
            print("[LOG] status_broadcaster ì¢…ë£Œë¨")
            break
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆì™¸ ì²˜ë¦¬
            print(f"[ERROR] status_broadcaster ì˜¤ë¥˜: {e}")
            await asyncio.sleep(1)  # ì˜¤ë¥˜ ì‹œ ì ì‹œ ëŒ€ê¸°


# cleanup ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
_cleanup_executed = False

def cleanup_and_exit():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬ ì‘ì—…"""
    global _cleanup_executed
    
    if _cleanup_executed:
        return
    
    _cleanup_executed = True
    print("[LOG] ì„œë²„ ì¢…ë£Œ ì¤‘...")
    print("[LOG] FastAPI ì„œë²„ ì¢…ë£Œ: ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ ë‹¤ìš´ë¡œë“œ ìŠ¤ë ˆë“œ ê´€ë¦¬ ëª©ë¡ ë¹„ì›€")
    download_manager.terminate_all_downloads()
    
    # ì§„í–‰ ì¤‘ì¸ ë‹¤ìš´ë¡œë“œë“¤ì„ ì •ì§€ ìƒíƒœë¡œ ë³€ê²½
    db = next(get_db())
    try:
        affected = db.query(DownloadRequest).filter(
            DownloadRequest.status.in_([StatusEnum.downloading, StatusEnum.proxying, StatusEnum.parsing])
        ).update({"status": "stopped"})
        db.commit()
        print(f"[LOG] ì„œë²„ ì¢…ë£Œ ì‹œ {affected}ê°œì˜ ì§„í–‰ ì¤‘ ë‹¤ìš´ë¡œë“œë¥¼ stoppedë¡œ ë³€ê²½")
    except Exception as e:
        print(f"[LOG] ì„œë²„ ì¢…ë£Œ ì‹œ ë‹¤ìš´ë¡œë“œ ìƒíƒœ ë³€ê²½ ì‹¤íŒ¨: {e}")
    finally:
        db.close()
    
    # ê°•ì œë¡œ ëª¨ë“  ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì¢…ë£Œ
    try:
        import threading
        for thread in threading.enumerate():
            if thread != threading.current_thread():
                print(f"[LOG] í™œì„± ìŠ¤ë ˆë“œ ë°œê²¬: {thread.name}")
        print(f"[LOG] ì´ {threading.active_count()}ê°œì˜ ìŠ¤ë ˆë“œ í™œì„±í™”")
    except Exception as e:
        print(f"[LOG] ìŠ¤ë ˆë“œ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    print("[LOG] ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

def signal_handler(signum, frame):
    """ì‹ í˜¸ ì²˜ë¦¬ê¸°"""
    print(f"\n[LOG] ì‹ í˜¸ {signum} ë°›ìŒ. ì„œë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    cleanup_and_exit()
    sys.exit(0)

# ì‹ í˜¸ ì²˜ë¦¬ê¸° ë“±ë¡ (ì¤‘ë³µ ë°©ì§€)
if not hasattr(cleanup_and_exit, '_handlers_registered'):
    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ì¢…ë£Œ ì‹ í˜¸
    atexit.register(cleanup_and_exit)  # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ
    cleanup_and_exit._handlers_registered = True



@app.websocket("/ws/status")
async def websocket_endpoint(websocket: WebSocket):
    connected = await manager.connect(websocket)
    if not connected:
        return  # ì—°ê²° ì œí•œìœ¼ë¡œ ì¸í•´ ì—°ê²° ì‹¤íŒ¨
    
    try:
        while True:
            await asyncio.sleep(30)  # keep alive - 30ì´ˆë¡œ ì¦ê°€
    except WebSocketDisconnect:
        print("[LOG] WebSocket ì •ìƒ ì—°ê²° í•´ì œ")
        manager.disconnect(websocket)
    except ConnectionClosedError:
        print("[LOG] WebSocket ì—°ê²°ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë¨")
        manager.disconnect(websocket)
    except ConnectionClosedOK:
        print("[LOG] WebSocket ì—°ê²°ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë¨")
        manager.disconnect(websocket)
    except asyncio.CancelledError:
        print("[LOG] WebSocket íƒœìŠ¤í¬ê°€ ì·¨ì†Œë¨")
        manager.disconnect(websocket)
    except Exception as e:
        print(f"[ERROR] WebSocket ì˜ˆì™¸: {e}")
        manager.disconnect(websocket)

def notify_status_update(db: Session, download_id: int, lang: str = "ko"):
    import json
    item = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
    if item:
        item_dict = item.as_dict()
        item_dict["status"] = item.status # Send raw status to frontend
        # Safely handle encoding for log messages
        try:
            print(f"[LOG] Notifying status update for {download_id}: {item_dict}")
            print(f"[LOG] WebSocket ì—°ê²° ìˆ˜: {len(manager.active_connections)}")
        except UnicodeEncodeError:
            print(f"[LOG] Notifying status update for {download_id}: (encoding error)")
        
        message = json.dumps({"type": "status_update", "data": item_dict}, ensure_ascii=False)
        status_queue.put(message)
        print(f"[LOG] ìƒíƒœ ì—…ë°ì´íŠ¸ ë©”ì‹œì§€ë¥¼ íì— ì¶”ê°€í•¨: {download_id} -> {item.status}")

async def notify_proxy_try(download_id: int, proxy: str):
    import json
    await manager.broadcast(json.dumps({"id": download_id, "proxy": proxy, "type": "proxy_try"}))

class DownloadRequestCreate(BaseModel):
    url: HttpUrl
    password: Optional[str] = None
    use_proxy: Optional[bool] = True


def parse_direct_link(url, password=None, proxies=None, req_id=None, use_proxy=True):
    """1fichierì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (ê°•í™”ëœ ë²„ì „)"""
    from core.parser import fichier_parser
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br, zstd',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
    }
    
    # ê°•í™”ëœ íŒŒì‹± ì‹œë„
    def try_advanced_parsing(scraper_or_session, method_name):
        """ê³ ê¸‰ íŒŒì‹± ì‹œë„"""
        try:
            # 1ë‹¨ê³„: GET ìš”ì²­ìœ¼ë¡œ ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ
            print(f"[LOG] {method_name} - ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ...")
            r1 = scraper_or_session.get(url, headers=headers, timeout=3)
            print(f"[LOG] {method_name} GET ì‘ë‹µ: {r1.status_code}")
            
            # HTTP ìƒíƒœ ì½”ë“œê°€ 500ì´ì–´ë„ HTML ì‘ë‹µì´ ìˆìœ¼ë©´ ê³„ì† ì§„í–‰
            if r1.status_code not in [200, 500]:
                print(f"[LOG] {method_name} GET ì‹¤íŒ¨: {r1.status_code}")
                return None
            elif r1.status_code == 500:
                print(f"[LOG] {method_name} GET 500 ì˜¤ë¥˜ì§€ë§Œ HTML ì‘ë‹µ ìˆìŒ, ê³„ì† ì§„í–‰...")
            
            # ëŒ€ê¸° ì‹œê°„ í™•ì¸ (JavaScript íƒ€ì´ë¨¸)
            timer_matches = re.findall(r'setTimeout\s*\([^,]+,\s*(\d+)', r1.text)
            wait_time = 0
            for match in timer_matches:
                time_ms = int(match)
                if 1000 <= time_ms <= 60000:  # 1ì´ˆ~60ì´ˆ
                    wait_time = max(wait_time, time_ms / 1000)
            
            if wait_time > 0:
                print(f"[LOG] {method_name} - ëŒ€ê¸° ì‹œê°„ ê°ì§€: {wait_time}ì´ˆ")
                time.sleep(min(wait_time, 30))  # ìµœëŒ€ 30ì´ˆ
            else:
                print(f"[LOG] {method_name} - ê¸°ë³¸ ëŒ€ê¸°: 3ì´ˆ")
                time.sleep(3)
            
            # 2ë‹¨ê³„: POST ë°ì´í„° êµ¬ì„±
            payload = {'dl_no_ssl': 'on', 'dlinline': 'on'}
            if password:
                payload['pass'] = password
            
            # ìˆ¨ê²¨ì§„ í•„ë“œ ì¶”ì¶œ
            hidden_fields = re.findall(r'<input[^>]*type=["\']hidden["\'][^>]*name=["\']([^"\']+)["\'][^>]*value=["\']([^"\']*)["\']', r1.text, re.IGNORECASE)
            for field_name, field_value in hidden_fields:
                payload[field_name] = field_value
                print(f"[LOG] {method_name} - ìˆ¨ê²¨ì§„ í•„ë“œ: {field_name}")
            
            # CSRF í† í° ì¶”ì¶œ
            csrf_patterns = [
                r'name=["\']_token["\'][^>]*value=["\']([^"\']+)["\']',
                r'value=["\']([^"\']+)["\'][^>]*name=["\']_token["\']'
            ]
            for pattern in csrf_patterns:
                match = re.search(pattern, r1.text, re.IGNORECASE)
                if match:
                    payload['_token'] = match.group(1)
                    print(f"[LOG] {method_name} - CSRF í† í° ì¶”ê°€")
                    break
            
            # 3ë‹¨ê³„: POST ìš”ì²­
            headers_post = headers.copy()
            headers_post['Referer'] = str(url)
            
            print(f"[LOG] {method_name} - POST ìš”ì²­...")
            r2 = scraper_or_session.post(url, data=payload, headers=headers_post, timeout=3)
            print(f"[LOG] {method_name} POST ì‘ë‹µ: {r2.status_code}")
            
            # HTTP ìƒíƒœ ì½”ë“œê°€ 500ì´ì–´ë„ HTML ì‘ë‹µì´ ìˆìœ¼ë©´ íŒŒì‹± ì‹œë„
            if r2.status_code in [200, 500]:
                if r2.status_code == 500:
                    print(f"[LOG] {method_name} POST 500 ì˜¤ë¥˜ì§€ë§Œ HTML ì‘ë‹µ ìˆìŒ, íŒŒì‹± ì‹œë„...")
                
                # íŒŒì„œë¡œ ë§í¬ ì¶”ì¶œ
                direct_link = fichier_parser.parse_download_link(r2.text, str(url))
                if direct_link and direct_link != str(url):
                    print(f"[LOG] {method_name}ë¡œ ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                    
                    # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    file_info = fichier_parser.extract_file_info(r2.text)
                    if file_info.get('name'):
                        print(f"[LOG] íŒŒì¼ëª…: {file_info['name']}")
                    if file_info.get('size'):
                        print(f"[LOG] íŒŒì¼ í¬ê¸°: {file_info['size']}")
                    
                    return direct_link
                else:
                    print(f"[LOG] {method_name}ì—ì„œ ìœ íš¨í•œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"[LOG] {method_name} POST ì‹¤íŒ¨: {r2.status_code}")
            
        except Exception as e:
            print(f"[LOG] {method_name} ì˜ˆì™¸: {e}")
        
        return None
    
    # í”„ë¡ì‹œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œë§Œ ë¡œì»¬ ì—°ê²° ì‹œë„
    if not use_proxy:
        print(f"[LOG] í”„ë¡ì‹œ ë¹„í™œì„±í™”. ë¡œì»¬ ì—°ê²°ë¡œ 1fichier íŒŒì‹± ì‹œë„...")
        try:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            scraper = cloudscraper.create_scraper(
                browser={
                    'browser': 'chrome',
                    'platform': 'windows',
                    'desktop': True
                }
            )
            
            result = try_advanced_parsing(scraper, "ë¡œì»¬ cloudscraper")
            if result:
                return result
        except Exception as e:
            print(f"[LOG] cloudscraper ì˜¤ë¥˜: {e}")
            
            # cloudscraper ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ requests ì‹œë„
            try:
                import requests
                session = requests.Session()
                session.verify = False
                result = try_advanced_parsing(session, "ë¡œì»¬ requests")
                if result:
                    return result
            except Exception as e2:
                print(f"[LOG] requests ì˜¤ë¥˜: {e2}")
        
        print(f"[LOG] ë¡œì»¬ ì—°ê²°ë¡œ íŒŒì‹± ì‹¤íŒ¨")
        return None
    
    # í”„ë¡ì‹œê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ í”„ë¡ì‹œë§Œ ì‚¬ìš©
    if proxies:
        print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œë¡œ íŒŒì‹± ì‹œë„: {proxies}")
        try:
            r = scraper.post(url, data=payload, headers=headers, proxies=proxies, timeout=3, verify=False)
            print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œ ì‘ë‹µì½”ë“œ: {r.status_code}")
            
            if r.status_code == 200:
                print(f"[LOG] HTML ì‘ë‹µ ê¸¸ì´: {len(r.text)} ë¬¸ì")
                
                # ìƒˆë¡œìš´ íŒŒì„œ ì‚¬ìš©
                direct_link = fichier_parser.parse_download_link(r.text, str(url))
                if direct_link:
                    print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œë¡œ ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                    
                    # íŒŒì¼ ì •ë³´ë„ ì¶”ì¶œ
                    file_info = fichier_parser.extract_file_info(r.text)
                    if file_info.get('name'):
                        print(f"[LOG] íŒŒì¼ëª…: {file_info['name']}")
                    if file_info.get('size'):
                        print(f"[LOG] íŒŒì¼ í¬ê¸°: {file_info['size']}")
                    
                    return direct_link
                else:
                    print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œ HTTP ì˜¤ë¥˜: {r.status_code}")
                
        except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, requests.exceptions.Timeout) as e:
            print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œ íƒ€ì„ì•„ì›ƒ: {e}")
            raise e  # í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ raise
        except requests.exceptions.ProxyError as e:
            print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œ ì—°ê²° ì˜¤ë¥˜: {e}")
            raise e  # í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ raise
        except Exception as e:
            print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œ ì˜ˆì™¸ ë°œìƒ: {e}")
            raise e
    
    print(f"[LOG] í”„ë¡ì‹œ íŒŒì‹± ì‹¤íŒ¨")
    return None

def is_direct_link_expired(direct_link, use_proxy=False, proxy_addr=None):
    """direct_linkê°€ ë§Œë£Œë˜ì—ˆëŠ”ì§€ ê°„ë‹¨íˆ ì²´í¬"""
    if not direct_link:
        return True
    
    # í”„ë¡ì‹œ ì„¤ì •
    proxies = None
    if use_proxy and proxy_addr:
        proxies = {
            'http': f'http://{proxy_addr}',
            'https': f'http://{proxy_addr}'
        }
    
    # 1fichier ë‹¤ìš´ë¡œë“œì— ì í•©í•œ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Accept': '*/*',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
    }
    
    try:
        # HEAD ìš”ì²­ìœ¼ë¡œ ë§í¬ ìœ íš¨ì„± í™•ì¸
        response = requests.head(direct_link, headers=headers, timeout=(1, 3), allow_redirects=True, proxies=proxies)
        print(f"[LOG] direct_link ìœ íš¨ì„± ê²€ì‚¬: {response.status_code}")
        
        if response.status_code in [200, 206]:  # 200 OK ë˜ëŠ” 206 Partial Content
            return False
        elif response.status_code in [403, 404, 410, 429]:  # ë§Œë£Œë˜ê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€, ìš”ì²­ ì œí•œ
            print(f"[LOG] direct_link ë§Œë£Œ ê°ì§€: {response.status_code}")
            return True
        else:
            print(f"[LOG] direct_link ìƒíƒœ í™•ì¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ: {response.status_code}")
            return True
    except Exception as e:
        print(f"[LOG] direct_link ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        return True

def get_or_parse_direct_link(req, proxies=None, use_proxy=True, force_reparse=False, proxy_addr=None):
    """ë‹¤ìš´ë¡œë“œ ìš”ì²­ì—ì„œ ì§ì ‘ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)"""
    
    # proxy_addrì´ ìˆìœ¼ë©´ proxies ìƒì„±
    if proxy_addr and use_proxy:
        proxies = {
            'http': f'http://{proxy_addr}',
            'https': f'http://{proxy_addr}'
        }
        # print(f"[LOG] í”„ë¡ì‹œ ì„¤ì •: {proxy_addr}")
    
    # ê°•ì œ ì¬íŒŒì‹±ì´ ìš”ì²­ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ ë§í¬ê°€ ì—†ëŠ” ê²½ìš°
    if force_reparse or not req.direct_link:
        print(f"[LOG] direct_link ìƒˆë¡œ íŒŒì‹± (force_reparse: {force_reparse}, proxy: {proxy_addr})")
        return parse_direct_link(req.url, req.password, proxies=proxies, req_id=req.id, use_proxy=use_proxy)
    
    # ê¸°ì¡´ ë§í¬ê°€ ìˆëŠ” ê²½ìš° ë§Œë£Œ ì—¬ë¶€ í™•ì¸
    if is_direct_link_expired(req.direct_link, use_proxy=use_proxy, proxy_addr=proxy_addr):
        print(f"[LOG] ê¸°ì¡´ direct_linkê°€ ë§Œë£Œë¨. ì¬íŒŒì‹± ì‹œì‘: {req.direct_link} (proxy: {proxy_addr})")
        return parse_direct_link(req.url, req.password, proxies=proxies, req_id=req.id, use_proxy=use_proxy)
    
    print(f"[LOG] ê¸°ì¡´ direct_link ì¬ì‚¬ìš©: {req.direct_link}")
    return req.direct_link

def exit_if_parent_dead():
    """ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ë©´ ìì‹ í”„ë¡œì„¸ìŠ¤ë„ ì¢…ë£Œí•˜ëŠ” í•¨ìˆ˜"""
    parent_pid = os.getppid()
    check_interval = int(os.getenv('PARENT_CHECK_INTERVAL', '5'))  # ê¸°ë³¸ 5ì´ˆ
    print(f"[LOG] ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ì²´í¬ ê°„ê²©: {check_interval}ì´ˆ)")
    
    while True:
        if os.getppid() != parent_pid:
            print("[LOG] ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë¨. ìì‹ í”„ë¡œì„¸ìŠ¤ë„ ì¢…ë£Œ.")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ëŠ” ìŠ¤í‚µ (ë¶€ëª¨ê°€ ì¢…ë£Œë˜ë©´ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
            os._exit(0)
        time.sleep(check_interval)

def cleanup_incomplete_file(file_path, is_complete=False):
    """ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ì§€ ì•Šì€ íŒŒì¼ ì •ë¦¬"""
    try:
        if file_path and os.path.exists(file_path):
            if not is_complete:
                # ë¶ˆì™„ì „í•œ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì€ .part í™•ì¥ì ì¶”ê°€í•˜ì—¬ êµ¬ë¶„
                part_path = str(file_path) + ".part"
                if not os.path.exists(part_path):
                    os.rename(file_path, part_path)
                    print(f"[LOG] ë¶ˆì™„ì „í•œ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì„ .partë¡œ ì´ë¦„ ë³€ê²½: {part_path}")
            else:
                # ì™„ë£Œëœ ë‹¤ìš´ë¡œë“œëŠ” .part í™•ì¥ì ì œê±°
                if str(file_path).endswith('.part'):
                    final_path = str(file_path)[:-5]  # .part ì œê±°
                    os.rename(file_path, final_path)
                    print(f"[LOG] ì™„ë£Œëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì—ì„œ .part ì œê±°: {final_path}")
    except Exception as e:
        print(f"[LOG] íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def download_1fichier_file_NEW_VERSION(request_id: int, lang: str = "ko", use_proxy: bool = True):
    # ë¶€ëª¨ ê°ì‹œ ìŠ¤ë ˆë“œ ì‹œì‘ (ìì‹ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ë™ì‘)
    if os.getppid() != os.getpid():
        threading.Thread(target=exit_if_parent_dead, daemon=True).start()
    
    print("=" * 80)
    print(f"[LOG] *** ìµœì‹  í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§ ë²„ì „ 2024 *** DOWNLOAD START")
    print(f"[LOG] Request ID: {request_id}, Lang: {lang}, Use Proxy: {use_proxy}")
    print(f"[LOG] Function called at: {time.strftime('%H:%M:%S')}")
    print("=" * 80)
    
    # ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    max_duration = 300  # 5ë¶„
    
    db = next(get_db()) # Get a new session for this background task
    req = None # Initialize req to None
    try:
        req = db.query(DownloadRequest).filter(DownloadRequest.id == request_id).first()
        if req is None:
            print(f"[LOG] DownloadRequest not found: {request_id}")
            return

        download_path = get_download_path()
        print(f"[LOG] Download path: {download_path}")

        # Check if file already exists to resume download
        base_filename = req.file_name if req is not None and req.file_name is not None else f"download_{request_id}"
        file_path = download_path / base_filename
        part_file_path = download_path / (base_filename + ".part")
        
        initial_downloaded_size = 0
        # .part íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
        if part_file_path.exists():
            file_path = part_file_path
            initial_downloaded_size = part_file_path.stat().st_size
            print(f"[LOG] Resuming download from .part file for {req.id} from {initial_downloaded_size} bytes. File: {file_path}")
        elif file_path.exists():
            initial_downloaded_size = file_path.stat().st_size
            print(f"[LOG] Resuming download for {req.id} from {initial_downloaded_size} bytes. File: {file_path}")
        else:
            # ìƒˆ ë‹¤ìš´ë¡œë“œëŠ” .part íŒŒì¼ë¡œ ì‹œì‘
            file_path = part_file_path
            print(f"[LOG] Starting new download for {req.id}. File: {file_path}")
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ DBì˜ downloaded_sizeë„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
            if req.downloaded_size > 0:
                print(f"[LOG] DB downloaded_size ì´ˆê¸°í™”: {req.downloaded_size} â†’ 0")
                req.downloaded_size = 0
        
        # Removed: req.status = "proxying" # Set status to proxying before parsing direct link
        # The status should be set by the calling endpoint (e.g., create_download_task, resume_download)
        db.commit()
        print(f"[LOG] Before notify (proxying): req.total_size={req.total_size}, req.downloaded_size={req.downloaded_size}")
        notify_status_update(db, int(getattr(req, 'id')), lang)
        print(f"[LOG] DB status updated to proxying for {req.id}")
        print(f"[LOG] Before get_or_parse_direct_link: req.total_size={req.total_size}, req.downloaded_size={req.downloaded_size}")
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        if time.time() - start_time > max_duration:
            raise TimeoutError("Download function timed out after 5 minutes")
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬ë§Œ ìœ ì§€
        
        # ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì „ ìƒíƒœ ì²´í¬ (í˜¹ì‹œ ì´ë¯¸ ì •ì§€ ìš”ì²­ì´ ìˆì—ˆëŠ”ì§€)
        db.refresh(req)
        if req.status == StatusEnum.stopped:
            print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì „ ì´ë¯¸ ì •ì§€ ìƒíƒœ ê°ì§€. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return
        
        # ê°„ë‹¨í•œ í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§
        print(f"[LOG] í”„ë¡ì‹œ ì‚¬ìš© ì—¬ë¶€: {use_proxy}")
        
        available_proxies = []
        if use_proxy:
            available_proxies = get_unused_proxies(db)
            print(f"[LOG] ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ: {len(available_proxies)}ê°œ")
        
        # ì´ì–´ë°›ê¸°ì¸ ê²½ìš° direct_link ì¬íŒŒì‹±
        force_reparse = initial_downloaded_size > 0
        if force_reparse:
            print(f"[LOG] ì´ì–´ë°›ê¸° ê°ì§€. direct_link ì¬íŒŒì‹± ìˆ˜í–‰")
        
        # Direct Link íŒŒì‹± with í”„ë¡ì‹œ ìˆœí™˜
        direct_link = None
        used_proxy_addr = None
        download_proxies = None
        
        if use_proxy and available_proxies:
            # í”„ë¡ì‹œ ìˆœí™˜í•˜ì—¬ direct_link íŒŒì‹±
            print(f"[LOG] *** í”„ë¡ì‹œ ìˆœí™˜ ì‹œì‘! ì´ {len(available_proxies)}ê°œ í”„ë¡ì‹œ ***")
            for i, proxy_addr in enumerate(available_proxies):
                # ìƒíƒœ ì²´í¬ - stoppedë©´ ì¤‘ë‹¨
                db.refresh(req)
                if req.status == StatusEnum.stopped:
                    print(f"[LOG] í”„ë¡ì‹œ íŒŒì‹± ì¤‘ ì •ì§€ ìš”ì²­ ê°ì§€. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    return
                
                try:
                    print(f"[LOG] Direct Link íŒŒì‹± - í”„ë¡ì‹œ {i+1}/{len(available_proxies)} ì‹œë„: {proxy_addr}")
                    
                    # í”„ë¡ì‹œ ì‹œë„ ì¤‘ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        import json
                        status_queue.put(json.dumps({
                            "type": "proxy_trying", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "parsing",
                                "current": i+1,
                                "total": len(available_proxies)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    
                    # í”„ë¡ì‹œ ì‹œë„ ì „ ìƒíƒœ ì¬ì²´í¬
                    db.refresh(req)
                    if req.status == StatusEnum.stopped:
                        print(f"[LOG] í”„ë¡ì‹œ íŒŒì‹± ì‹œë„ ì „ ì •ì§€ ìš”ì²­ ê°ì§€. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        return
                    
                    direct_link = get_or_parse_direct_link(req, use_proxy=True, force_reparse=force_reparse, proxy_addr=proxy_addr)
                    
                    # íŒŒì‹± ì„±ê³µ í›„ ë‹¤ìŒ í”„ë¡ì‹œë¡œ
                    
                    if direct_link:
                        used_proxy_addr = proxy_addr
                        download_proxies = {
                            'http': f'http://{proxy_addr}',
                            'https': f'http://{proxy_addr}'
                        }
                        # mark_proxy_used(db, proxy_addr, success=True)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€: ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ì‹œì—ë§Œ ê¸°ë¡
                        print(f"[LOG] Direct Link íŒŒì‹± ì„±ê³µ - í”„ë¡ì‹œ: {proxy_addr}")
                        
                        # í”„ë¡ì‹œ ì„±ê³µ ìƒíƒœ WebSocket ì „ì†¡
                        try:
                            status_queue.put(json.dumps({
                                "type": "proxy_success", 
                                "data": {
                                    "proxy": proxy_addr,
                                    "step": "parsing"
                                }
                            }, ensure_ascii=False))
                        except:
                            pass
                        break
                except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, 
                        requests.exceptions.Timeout, requests.exceptions.ProxyError) as e:
                    print(f"[LOG] Direct Link íŒŒì‹± ì‹¤íŒ¨ - í”„ë¡ì‹œ {proxy_addr}: {e}")
                    # mark_proxy_used(db, proxy_addr, success=False)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
                    
                    # ì˜ˆì™¸ ë°œìƒ ì‹œ ë‹¤ìŒ í”„ë¡ì‹œë¡œ
                    
                    # í”„ë¡ì‹œ ì‹¤íŒ¨ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        status_queue.put(json.dumps({
                            "type": "proxy_failed", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "parsing",
                                "error": str(e)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    continue
                except Exception as e:
                    print(f"[LOG] Direct Link íŒŒì‹± ì˜¤ë¥˜ - í”„ë¡ì‹œ {proxy_addr}: {e}")
                    # mark_proxy_used(db, proxy_addr, success=False)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
                    
                    # í”„ë¡ì‹œ ì‹¤íŒ¨ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        status_queue.put(json.dumps({
                            "type": "proxy_failed", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "parsing",
                                "error": str(e)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    continue
        else:
            # í”„ë¡ì‹œ ì—†ì´ ì‹œë„
            direct_link = get_or_parse_direct_link(req, use_proxy=False, force_reparse=force_reparse, proxy_addr=None)
        print(f"[LOG] After get_or_parse_direct_link: direct_link={direct_link}, req.total_size={req.total_size}, req.downloaded_size={req.downloaded_size}")
        print(f"[LOG] Direct link for {req.id}: {direct_link}")

        if not direct_link:
            raise Exception("Cannot find download link from 1fichier. Site structure may have changed or proxy issue.")
        
        # ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        req.direct_link = direct_link
        db.commit()

        setattr(req, "status", StatusEnum.downloading) # Set status to downloading after direct link is found
        db.commit()
        notify_status_update(db, int(getattr(req, 'id')), lang)
        print(f"[LOG] DB status updated to downloading for {req.id}")

        # ë‹¨ìˆœí•œ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì¬ì‹œë„ë§Œ í¬í•¨)
        def simple_download(url, headers, proxies, max_retries=2):
            """ë‹¨ìˆœ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ - ProxyError ì‹œ ì¦‰ì‹œ ì‹¤íŒ¨"""
            range_removed = False
            
            for attempt in range(max_retries):
                # ì¬ì‹œë„ëŠ” ë¹ ë¥´ê²Œ ì§„í–‰
                try:
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries} (í”„ë¡ì‹œ: {proxies})")
                    current_headers = headers.copy()
                    
                    # HTTP ìš”ì²­ì€ ë¹ ë¥´ê²Œ ì§„í–‰
                    
                    response = requests.get(url, stream=True, allow_redirects=True, headers=current_headers, proxies=proxies, timeout=(1, 5))
                    print(f"[LOG] ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
                    
                    # 409 ì—ëŸ¬ ì²˜ë¦¬ (Range í—¤ë” ì œê±°)
                    if response.status_code == 409 and "Range" in current_headers:
                        print(f"[LOG] 409 ì—ëŸ¬ - Range í—¤ë” ì œê±° í›„ ì¬ì‹œë„")
                        current_headers.pop("Range", None)
                        range_removed = True
                        response = requests.get(url, stream=True, allow_redirects=True, headers=current_headers, proxies=proxies, timeout=(1, 5))
                    
                    response.raise_for_status()
                    return response, range_removed
                    
                except requests.exceptions.HTTPError as e:
                    if e.response and e.response.status_code in [403, 404, 410]:
                        # ë§í¬ ë§Œë£Œ - ì¬íŒŒì‹± í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ë°”ë¡œ ì‹¤íŒ¨
                        print(f"[LOG] HTTP {e.response.status_code} - ë§í¬ ë§Œë£Œ")
                        raise e
                    if attempt < max_retries - 1:
                        print(f"[LOG] HTTP ì—ëŸ¬ ì¬ì‹œë„...")
                        time.sleep(1)
                        continue
                    raise e
                    
                except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, 
                        requests.exceptions.Timeout, requests.exceptions.ProxyError) as e:
                    # í”„ë¡ì‹œ/ì—°ê²° ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨ (ë‹¤ìŒ í”„ë¡ì‹œë¡œ)
                    print(f"[LOG] ì—°ê²°/í”„ë¡ì‹œ ì—ëŸ¬: {e}")
                    raise e
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"[LOG] ì¼ë°˜ ì—ëŸ¬ ì¬ì‹œë„: {e}")
                        time.sleep(1)
                        continue
                    raise e
            
            raise Exception("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")

        # ì´ì–´ë°›ê¸°ë¥¼ ìœ„í•œ HEAD ìš”ì²­ìœ¼ë¡œ ì„œë²„ Range ì§€ì› í™•ì¸
        resume_supported = False
        server_file_size = None
        
        if initial_downloaded_size > 0:
            try:
                print(f"[LOG] HEAD ìš”ì²­ìœ¼ë¡œ ì„œë²„ íŒŒì¼ ì •ë³´ í™•ì¸...")
                head_response = requests.head(str(direct_link), allow_redirects=True, proxies=download_proxies, timeout=(2, 5))
                print(f"[LOG] HEAD ì‘ë‹µ: {head_response.status_code}")
                print(f"[LOG] HEAD í—¤ë”: {head_response.headers}")
                
                # Accept-Ranges í—¤ë” í™•ì¸
                accept_ranges = head_response.headers.get('Accept-Ranges', '').lower()
                server_file_size = head_response.headers.get('Content-Length')
                
                if accept_ranges == 'bytes' and server_file_size:
                    server_file_size = int(server_file_size)
                    print(f"[LOG] ì„œë²„ íŒŒì¼ í¬ê¸°: {server_file_size}, í˜„ì¬ ë‹¤ìš´ë¡œë“œëœ í¬ê¸°: {initial_downloaded_size}")
                    
                    if initial_downloaded_size < server_file_size:
                        resume_supported = True
                        print(f"[LOG] ì´ì–´ë°›ê¸° ì§€ì›ë¨")
                    elif initial_downloaded_size >= server_file_size:
                        print(f"[LOG] íŒŒì¼ì´ ì´ë¯¸ ì™„ì „íˆ ë‹¤ìš´ë¡œë“œë¨")
                        # ì™„ë£Œ ì²˜ë¦¬
                        setattr(req, "status", StatusEnum.done)
                        setattr(req, "downloaded_size", server_file_size)
                        setattr(req, "total_size", server_file_size)
                        db.commit()
                        cleanup_incomplete_file(file_path, is_complete=True)
                        notify_status_update(db, int(getattr(req, 'id')), lang)
                        return
                    else:
                        print(f"[LOG] ë¡œì»¬ íŒŒì¼ì´ ì„œë²„ íŒŒì¼ë³´ë‹¤ í¼. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ")
                        initial_downloaded_size = 0
                        file_path = part_file_path  # .part íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹œì‘
                else:
                    print(f"[LOG] ì„œë²„ê°€ Range ìš”ì²­ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ (Accept-Ranges: {accept_ranges})")
                    
            except Exception as e:
                print(f"[LOG] HEAD ìš”ì²­ ì‹¤íŒ¨: {e}. ì´ì–´ë°›ê¸° ë¹„í™œì„±í™”")

        # Range í—¤ë” ì„¤ì •
        headers = {}
        if resume_supported and initial_downloaded_size > 0:
            headers["Range"] = f"bytes={initial_downloaded_size}-"
            print(f"[LOG] ì´ì–´ë°›ê¸° ìš”ì²­ í—¤ë”: {headers}")
        else:
            if initial_downloaded_size > 0:
                print(f"[LOG] ì´ì–´ë°›ê¸° ì‹¤íŒ¨. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ")
                initial_downloaded_size = 0
                file_path = part_file_path  # .part íŒŒì¼ë¡œ ë‹¤ì‹œ ì‹œì‘
        
        # ê°œì„ ëœ ë‹¤ìš´ë¡œë“œ í—¤ë”
        download_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'cross-site',
        }
        
        # Range í—¤ë” ì¶”ê°€ (ì´ì–´ë°›ê¸°ìš©)
        if resume_supported and initial_downloaded_size > 0:
            download_headers["Range"] = f"bytes={initial_downloaded_size}-"
            print(f"[LOG] ì´ì–´ë°›ê¸° ìš”ì²­ í—¤ë”: Range={download_headers['Range']}")

        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìš”ì²­ with í”„ë¡ì‹œ ìˆœí™˜
        r = None
        range_was_removed = False
        
        if use_proxy and available_proxies:
            # ë‚¨ì€ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œë“¤ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„
            remaining_proxies = get_unused_proxies(db)
            print(f"[LOG] ë‹¤ìš´ë¡œë“œìš© ë‚¨ì€ í”„ë¡ì‹œ: {len(remaining_proxies)}ê°œ")
            
            for i, proxy_addr in enumerate(remaining_proxies):
                # ë‹¤ìš´ë¡œë“œ í”„ë¡ì‹œ ìˆœí™˜ ì‹œ ìƒíƒœ ì²´í¬ (ì£¼ìš” ì¤‘ë‹¨ì )
                db.refresh(req)
                if req.status == StatusEnum.stopped:
                    print(f"[LOG] í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ ì¤‘ ì •ì§€ ìš”ì²­ ê°ì§€. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    cleanup_incomplete_file(file_path, is_complete=False)
                    return
                
                try:
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ - í”„ë¡ì‹œ {i+1}/{len(remaining_proxies)} ì‹œë„: {proxy_addr}")
                    
                    # í”„ë¡ì‹œ ì‹œë„ ì¤‘ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        import json
                        status_queue.put(json.dumps({
                            "type": "proxy_trying", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "downloading",
                                "current": i+1,
                                "total": len(remaining_proxies)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    
                    current_proxies = {
                        'http': f'http://{proxy_addr}',
                        'https': f'http://{proxy_addr}'
                    }
                    
                    # ë‹¤ìš´ë¡œë“œ ì‹œë„
                    
                    r, range_was_removed = simple_download(str(direct_link), download_headers, current_proxies)
                    
                    # ë‹¤ìš´ë¡œë“œ ì„±ê³µ ì‹œ ì™„ë£Œ
                    
                    # ì„±ê³µí•˜ë©´ í”„ë¡ì‹œ ê¸°ë¡í•˜ê³  ì¤‘ë‹¨
                    # mark_proxy_used(db, proxy_addr, success=True)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€: ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ì‹œì—ë§Œ ê¸°ë¡
                    download_proxies = current_proxies
                    used_proxy_addr = proxy_addr
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì„±ê³µ - í”„ë¡ì‹œ: {proxy_addr}")
                    
                    # í”„ë¡ì‹œ ì„±ê³µ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        status_queue.put(json.dumps({
                            "type": "proxy_success", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "downloading"
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    break
                    
                except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout, 
                        requests.exceptions.Timeout, requests.exceptions.ProxyError) as e:
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - í”„ë¡ì‹œ {proxy_addr}: {e}")
                    # mark_proxy_used(db, proxy_addr, success=False)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
                    
                    # ì˜ˆì™¸ ë°œìƒ ì‹œ ë‹¤ìŒ í”„ë¡ì‹œë¡œ
                    
                    # í”„ë¡ì‹œ ì‹¤íŒ¨ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        status_queue.put(json.dumps({
                            "type": "proxy_failed", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "downloading",
                                "error": str(e)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    continue
                    
                except Exception as e:
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜ - í”„ë¡ì‹œ {proxy_addr}: {e}")
                    # mark_proxy_used(db, proxy_addr, success=False)  # ì¤‘ë³µ ê¸°ë¡ ë°©ì§€
                    
                    # í”„ë¡ì‹œ ì‹¤íŒ¨ ìƒíƒœ WebSocket ì „ì†¡
                    try:
                        status_queue.put(json.dumps({
                            "type": "proxy_failed", 
                            "data": {
                                "proxy": proxy_addr,
                                "step": "downloading",
                                "error": str(e)
                            }
                        }, ensure_ascii=False))
                    except:
                        pass
                    continue
            
            # ëª¨ë“  í”„ë¡ì‹œ ì‹¤íŒ¨í•œ ê²½ìš°
            if r is None:
                print(f"[LOG] ëª¨ë“  í”„ë¡ì‹œ ì‹¤íŒ¨. ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ ì‹œë„")
                r, range_was_removed = simple_download(str(direct_link), download_headers, None)
        else:
            # í”„ë¡ì‹œ ì—†ì´ ë‹¤ìš´ë¡œë“œ
            print(f"[LOG] ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ì‹œë„")
            r, range_was_removed = simple_download(str(direct_link), download_headers, download_proxies)
        
        # 409 ì—ëŸ¬ë¡œ ì¸í•´ Range í—¤ë”ê°€ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if range_was_removed:
            print(f"[LOG] 409 ì—ëŸ¬ë¡œ ì¸í•´ Range í—¤ë”ê°€ ì œê±°ë¨. ì „ì²´ ë‹¤ìš´ë¡œë“œë¡œ ë³€ê²½")
            initial_downloaded_size = 0
            file_path = part_file_path  # .part íŒŒì¼ë¡œ ë³€ê²½
            resume_supported = False
            # ê¸°ì¡´ .part íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ (ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘)
            if file_path.exists():
                print(f"[LOG] ê¸°ì¡´ .part íŒŒì¼ ì‚­ì œ: {file_path}")
                file_path.unlink()
        
        with r:
            print(f"[LOG] HTTP GET request sent for {req.id}. Status code: {r.status_code}")
            print(f"[LOG] Response headers: {r.headers}")
            print(f"[LOG] Content-Length header: {r.headers.get('Content-Length')}")
            print(f"[LOG] Content-Range header: {r.headers.get('Content-Range')}")
            r.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

            file_name = None
            if "Content-Disposition" in r.headers:
                # Try to extract filename from Content-Disposition header
                cd = r.headers["Content-Disposition"]
                file_name = re.findall(r"filename\*=UTF-8''(.+?)", cd)
                if not file_name:
                    file_name = re.findall(r"filename=\"(.*?)\"", cd)
                if file_name:
                    file_name = unquote(file_name[0])
            
            if not file_name:
                # Fallback to filename from URL
                file_name = os.path.basename(urlparse(str(direct_link)).path)
                if not file_name:
                    file_name = f"download_{request_id}" # Generic name if all else fails
            print(f"[LOG] Determined file name: {file_name}")

            # If resuming, and file_name was not set, set it now
            if req.file_name is None:
                setattr(req, "file_name", file_name)
                db.commit()

            # Get total size from Content-Length or Content-Range header
            total_size = 0
            if "Content-Range" in r.headers:
                # Parse Content-Range: bytes 200-1000/1001
                content_range = r.headers["Content-Range"]
                match = re.search(r"bytes \d+-\d+/(\d+)", content_range)
                if match:
                    total_size = int(match.group(1))
                    print(f"[LOG] Range ì‘ë‹µì—ì„œ ì „ì²´ íŒŒì¼ í¬ê¸°: {total_size}")
            elif "Content-Length" in r.headers:
                content_length = int(r.headers["Content-Length"])
                if initial_downloaded_size > 0:
                    # ì´ì–´ë°›ê¸°ì¸ ê²½ìš° ì „ì²´ í¬ê¸°ëŠ” ê¸°ì¡´ + ì¶”ê°€ë¡œ ë°›ì„ í¬ê¸°
                    total_size = initial_downloaded_size + content_length
                    print(f"[LOG] ì´ì–´ë°›ê¸°: ê¸°ì¡´ {initial_downloaded_size} + ì¶”ê°€ {content_length} = ì „ì²´ {total_size}")
                else:
                    # ìƒˆ ë‹¤ìš´ë¡œë“œì¸ ê²½ìš°
                    total_size = content_length
                    print(f"[LOG] ìƒˆ ë‹¤ìš´ë¡œë“œ ì „ì²´ í¬ê¸°: {total_size}")
            
            # ì„œë²„ì—ì„œ ë°›ì€ íŒŒì¼ í¬ê¸°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if server_file_size and server_file_size > total_size:
                total_size = server_file_size
                print(f"[LOG] HEAD ìš”ì²­ì—ì„œ ë°›ì€ íŒŒì¼ í¬ê¸° ì‚¬ìš©: {total_size}")

            downloaded_size = initial_downloaded_size

            setattr(req, "total_size", total_size)
            db.commit()
            notify_status_update(db, int(getattr(req, 'id')), lang) # Call notify_status_update here
            print(f"[LOG] DB file_name and total_size updated for {req.id}")

            # Open file in append mode if resuming, else write mode
            mode = "ab" if initial_downloaded_size > 0 and resume_supported else "wb"
            print(f"[LOG] Opening file in mode: {mode} (resume_supported: {resume_supported})")
            
            # ì²˜ìŒë¶€í„° ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²½ìš° ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
            if mode == "wb" and file_path.exists():
                print(f"[LOG] ê¸°ì¡´ íŒŒì¼ ì‚­ì œ í›„ ìƒˆë¡œ ì‹œì‘: {file_path}")
                file_path.unlink()
            with open(str(file_path), mode) as f:
                chunk_count = 0
                last_status_check = time.time()
                status_check_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ìƒíƒœ ì²´í¬
                
                for chunk in r.iter_content(chunk_size=8192):
                    chunk_count += 1
                    current_time = time.time()
                    
                    # íƒ€ì„ì•„ì›ƒ ì²´í¬
                    if current_time - start_time > max_duration:
                        raise TimeoutError("Download function timed out after 5 minutes")
                    
                    # DB ì²´í¬ëŠ” í”„ë¡ì‹œ ìˆœí™˜í•  ë•Œë§Œ (ì„±ëŠ¥ í–¥ìƒ)
                    
                    # ìƒì„¸ ë¡œê¹…ì€ ê°€ë”ë§Œ (ì„±ëŠ¥ì„ ìœ„í•´)
                    should_log_status = (
                        current_time - last_status_check >= status_check_interval or
                        chunk_count % 128 == 0  # 1MBë§ˆë‹¤
                    )
                    
                    if should_log_status:
                        print(f"[LOG] Download {req.id} status: {req.status}, chunk: {chunk_count}")
                        last_status_check = current_time
                    
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    setattr(req, "downloaded_size", downloaded_size)
                    db.commit()
                    # Update status every 1MB or 10% of total size, whichever is smaller
                    if total_size > 0 and (downloaded_size % (1024 * 1024) == 0 or downloaded_size * 10 // total_size != (downloaded_size - len(chunk)) * 10 // total_size):
                        notify_status_update(db, int(getattr(req, 'id')), lang)
                print(f"[LOG] Finished writing all chunks for {req.id}")

        setattr(req, "status", StatusEnum.done)
        setattr(req, "downloaded_size", total_size) # Ensure downloaded_size is total_size on completion
        db.commit()
        notify_status_update(db, int(getattr(req, 'id')), lang)
        
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ì‹œ .part í™•ì¥ì ì œê±°
        cleanup_incomplete_file(file_path, is_complete=True)
        
        # í”„ë¡ì‹œ ì‚¬ìš© ì„±ê³µ ê¸°ë¡
        if used_proxy_addr:
            mark_proxy_used(db, used_proxy_addr, True)
            print(f"[LOG] í”„ë¡ì‹œ {used_proxy_addr} ë‹¤ìš´ë¡œë“œ ì„±ê³µ ê¸°ë¡ë¨")
        
        try:
            print(f"[LOG] Download completed: {req.url}")
        except UnicodeEncodeError:
            print(f"[LOG] Download completed: (encoding error)")

    except requests.exceptions.RequestException as e:
        error_message = f"Network or HTTP error: {e}"
        # Clean error message to prevent encoding issues
        error_message = error_message.encode('ascii', 'ignore').decode('ascii')
        try:
            print(f"[LOG] Download {request_id} failed due to RequestException: {error_message}")
        except UnicodeEncodeError:
            print(f"[LOG] Download {request_id} failed due to RequestException: (encoding error)")
        # í”„ë¡ì‹œ ì‚¬ìš© ì‹¤íŒ¨ ê¸°ë¡ (ìµœì¢… ì‹¤íŒ¨ë§Œ ê¸°ë¡)
        if used_proxy_addr:
            mark_proxy_used(db, used_proxy_addr, False)
            print(f"[LOG] í”„ë¡ì‹œ {used_proxy_addr} ë‹¤ìš´ë¡œë“œ ìµœì¢… ì‹¤íŒ¨ ê¸°ë¡ë¨")
        
        if req: # Ensure req is defined before accessing its attributes
            setattr(req, "status", StatusEnum.failed)
            setattr(req, "error", error_message)
            db.commit()
            notify_status_update(db, int(getattr(req, 'id')), lang)
            
            # ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬
            if 'file_path' in locals():
                cleanup_incomplete_file(file_path, is_complete=False)
        else:
            print(f"[LOG] Critical: req was None when RequestException occurred for download_id {request_id}")
    except TimeoutError as e:
        error_message = f"Download timed out: {str(e)}"
        # Clean error message to prevent encoding issues
        error_message = error_message.encode('ascii', 'ignore').decode('ascii')
        try:
            print(f"[LOG] Download {request_id} failed due to TimeoutError: {error_message}")
        except UnicodeEncodeError:
            print(f"[LOG] Download {request_id} failed due to TimeoutError: (encoding error)")
        # í”„ë¡ì‹œ ì‚¬ìš© ì‹¤íŒ¨ ê¸°ë¡ (ìµœì¢… ì‹¤íŒ¨ë§Œ ê¸°ë¡)
        if used_proxy_addr:
            mark_proxy_used(db, used_proxy_addr, False)
            print(f"[LOG] í”„ë¡ì‹œ {used_proxy_addr} íƒ€ì„ì•„ì›ƒ ìµœì¢… ì‹¤íŒ¨ ê¸°ë¡ë¨")
        
        if req: # Ensure req is defined before accessing its attributes
            setattr(req, "status", StatusEnum.failed)
            setattr(req, "error", error_message)
            db.commit()
            notify_status_update(db, int(getattr(req, 'id')), lang)
            
            # íƒ€ì„ì•„ì›ƒëœ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬
            if 'file_path' in locals():
                cleanup_incomplete_file(file_path, is_complete=False)
        else:
            print(f"[LOG] Critical: req was None when TimeoutError occurred for download_id {request_id}")
    except Exception as e:
        error_message = str(e)
        # Clean error message to prevent encoding issues
        error_message = error_message.encode('ascii', 'ignore').decode('ascii')
        try:
            print(f"[LOG] Download {request_id} failed due to general Exception: {error_message}")
        except UnicodeEncodeError:
            print(f"[LOG] Download {request_id} failed due to general Exception: (encoding error)")
        
        # í”„ë¡ì‹œ ì‚¬ìš© ì‹¤íŒ¨ ê¸°ë¡ (ìµœì¢… ì‹¤íŒ¨ë§Œ ê¸°ë¡)
        if used_proxy_addr:
            mark_proxy_used(db, used_proxy_addr, False)
            print(f"[LOG] í”„ë¡ì‹œ {used_proxy_addr} ì¼ë°˜ ì—ëŸ¬ ìµœì¢… ì‹¤íŒ¨ ê¸°ë¡ë¨")
        
        if req: # Ensure req is defined before accessing its attributes
            setattr(req, "status", StatusEnum.failed)
            setattr(req, "error", error_message)
            db.commit()
            notify_status_update(db, int(getattr(req, 'id')), lang)
            
            # ì¼ë°˜ ì˜ˆì™¸ë¡œ ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë¦¬
            if 'file_path' in locals():
                cleanup_incomplete_file(file_path, is_complete=False)
        else:
            print(f"[LOG] Critical: req was None when general Exception occurred for download_id {request_id}")
    finally:
        db.close() # Ensure the session is closed
        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ë§¤ë‹ˆì €ì—ì„œ ì œê±°
        # download_manager.cleanup_completed() # This line is no longer needed as multiprocessing handles cleanup


status_map = {
    "pending": "download_pending",
    "downloading": "download_downloading",
    "paused": "download_paused",
    "proxying": "download_proxying",
    "done": "download_done",
    "failed": "download_failed"
}

def get_lang(request: Request):
    return request.headers.get("accept-language", "ko")[:2]

# Request = Depends()ê°€ ë¶™ì€ ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ë¥¼ main.pyì—ì„œ ì™„ì „íˆ ì‚­ì œ
# websocket, SPA ë¼ìš°íŒ…, app ì„¤ì • ë“±ë§Œ ë‚¨ê¹€

@api_router.get("/proxies/")
def get_proxies(req: Request, db: Session = Depends(get_db)):
    user_proxies = get_user_proxy_list(db)
    return user_proxies

@api_router.post("/proxies/")
def add_proxy(req: Request, proxy: str = Body(..., embed=True)):
    try:
        import os
        proxy_path = "/tmp/proxies.txt" if os.path.exists("/tmp") else "proxies.txt"
        with open(proxy_path, "a") as f:
            f.write(proxy + "\n")
    except PermissionError:
        print(f"[WARN] Cannot write to proxy file, proxy not saved: {proxy}")
    return {"message": "Proxy added successfully"}

@api_router.delete("/proxies/")
def delete_proxy(req: Request, proxy: str = Body(..., embed=True)):
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. proxy_manager.pyì˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    return {"success": False, "error": "Deprecated endpoint. Use /api/proxies instead."}
    if proxy in proxies:
        proxies.remove(proxy)
        try:
            import os
            proxy_path = "/tmp/proxies.txt" if os.path.exists("/tmp") else "proxies.txt"
            with open(proxy_path, "w") as f:
                for p in proxies:
                    f.write(p + "\n")
        except PermissionError:
            print(f"[WARN] Cannot write to proxy file, proxy not removed: {proxy}")
        return {"message": "Proxy deleted successfully"}
    else:
        raise HTTPException(status_code=404, detail="Proxy not found")

@api_router.get("/settings")
def get_settings_endpoint(req: Request):
    config = get_config()
    # config.jsonì— ì €ì¥ëœ ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return config

@api_router.get("/default_download_path")
async def get_default_download_path():
    """ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ë°˜í™˜ - í•­ìƒ /downloads"""
    return {"path": "/downloads"}

@api_router.post("/select_folder")
async def select_folder(req: Request):
    """í´ë” ì„ íƒ ëŒ€í™”ìƒìë¥¼ ì—´ê³  ì„ íƒëœ ê²½ë¡œë¥¼ ë°˜í™˜"""
    import os
    import platform
    import threading
    import asyncio
    
    # ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ í´ë”
    if platform.system() == "Windows":
        downloads_path = os.path.join(os.path.expanduser("~"), "Downloads")
    else:
        downloads_path = os.path.join(os.path.expanduser("~"), "Downloads")
    
    def open_folder_dialog():
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í´ë” ì„ íƒ ëŒ€í™”ìƒì ì‹¤í–‰"""
        if not GUI_AVAILABLE:
            print("[INFO] GUI not available in this environment, using default downloads path")
            return downloads_path
            
        try:
            root = tk.Tk()
            root.withdraw()
            root.wm_attributes('-topmost', 1)
            folder_path = filedialog.askdirectory(
                title="ë‹¤ìš´ë¡œë“œ í´ë” ì„ íƒ",
                initialdir=downloads_path
            )
            root.destroy()
            return folder_path if folder_path else downloads_path
        except Exception as e:
            print(f"[ERROR] tkinter folder dialog failed: {e}")
            return downloads_path
    
    try:
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ì„œë²„ ë¸”ë¡œí‚¹ ë°©ì§€
        loop = asyncio.get_event_loop()
        selected_path = await loop.run_in_executor(None, open_folder_dialog)
        return {"path": selected_path}
    except Exception as e:
        print(f"[ERROR] Folder selection error: {e}")
        return {"path": downloads_path}

@api_router.post("/settings")
def update_settings_endpoint(settings: dict, req: Request):
    try:
        print(f"[LOG] Received settings to save: {settings}")
        if 'download_path' in settings and settings['download_path'] is not None:
            # ì ˆëŒ€ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©, ìƒëŒ€ê²½ë¡œë§Œ resolve
            download_path = settings['download_path']
            if not Path(download_path).is_absolute():
                download_path = str(Path(download_path).resolve())
            settings['download_path'] = download_path
            # Create the directory if it doesn't exist
            try:
                Path(settings['download_path']).mkdir(parents=True, exist_ok=True)
            except PermissionError:
                print(f"[WARN] Cannot create directory: {settings['download_path']}")
        save_config(settings)
        return {"message": "Settings updated successfully"}
    except Exception as e:
        print(f"[ERROR] Failed to save settings: {e}")
        raise HTTPException(status_code=500, detail="Failed to save settings")

@api_router.get("/locales/{lang}.json")
async def serve_locale_file(lang: str, req: Request):
    """ìºì‹œëœ ë²ˆì—­ ë°ì´í„°ë¥¼ ë°˜í™˜"""
    translations = get_translations(lang)
    if translations:
        return JSONResponse(content=translations)
    raise HTTPException(status_code=404, detail="Locale not found")

@api_router.get("/downloads/active")
def get_active_downloads(req: Request):
    """í™œì„± ë‹¤ìš´ë¡œë“œ ëª©ë¡ ë°˜í™˜"""
    active_downloads = list(download_manager.active_downloads.keys())
    return {"active_downloads": active_downloads, "count": len(active_downloads)}

@api_router.post("/downloads/cancel/{download_id}")
def cancel_download(download_id: int, req: Request, db: Session = Depends(get_db)):
    """ë‹¤ìš´ë¡œë“œ ê°•ì œ ì·¨ì†Œ"""
    lang = get_lang(req)
    item = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
    if item is None:
        raise HTTPException(status_code=404, detail=get_message("download_not_found", lang))
    
    # ìŠ¤ë ˆë“œ ì·¨ì†Œ
    if download_manager.is_download_active(download_id):
        download_manager.cancel_download(download_id)
        print(f"[LOG] Download {download_id} forcefully cancelled")
    
    # ìƒíƒœë¥¼ failedë¡œ ë³€ê²½
    setattr(item, "status", StatusEnum.failed)
    setattr(item, "error", "Download cancelled by user")
    db.commit()
    notify_status_update(db, int(getattr(item, 'id')), lang)
    
    return {"id": item.id, "status": item.status, "message": "Download cancelled"}

@api_router.get("/proxy-status")
def get_proxy_status(req: Request, db: Session = Depends(get_db)):
    """í”„ë¡ì‹œ ì‚¬ìš© í˜„í™© ì¡°íšŒ"""
    try:
        # ìµœê·¼ 24ì‹œê°„ ë‚´ ì‚¬ìš©ëœ í”„ë¡ì‹œë“¤
        cutoff_time = datetime.datetime.utcnow() - datetime.timedelta(hours=24)
        
        used_proxies = db.query(ProxyStatus).filter(
            ProxyStatus.last_used_at > cutoff_time
        ).order_by(desc(ProxyStatus.last_used_at)).all()
        
        # ì „ì²´ í”„ë¡ì‹œ ê°œìˆ˜ (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡ì‹œ)
        unused_proxies = get_unused_proxies(db)
        all_proxies = get_user_proxy_list(db)  # ì‹¤ì œ ì „ì²´ í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸ í¬í•¨
        
        total_proxies = len(all_proxies)
        available_count = len(unused_proxies)
        used_count = total_proxies - available_count
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = len([p for p in used_proxies if p.last_status == 'success'])
        fail_count = len([p for p in used_proxies if p.last_status == 'fail'])
        
        proxy_list = []
        for proxy in used_proxies:
            proxy_list.append({
                "address": f"{proxy.ip}:{proxy.port}",
                "last_used": proxy.last_used_at.isoformat() if proxy.last_used_at else None,
                "status": proxy.last_status,
                "last_failed": proxy.last_failed_at.isoformat() if proxy.last_failed_at else None
            })
        
        return {
            "total_proxies": total_proxies,
            "used_proxies": used_count,
            "available_proxies": available_count,
            "success_count": success_count,
            "fail_count": fail_count,
            "used_proxy_list": proxy_list
        }
        
    except Exception as e:
        print(f"[ERROR] í”„ë¡ì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="í”„ë¡ì‹œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨")

@api_router.post("/proxy-status/reset")
def reset_proxy_status(req: Request, db: Session = Depends(get_db)):
    """í”„ë¡ì‹œ ì‚¬ìš© ê¸°ë¡ ì´ˆê¸°í™”"""
    try:
        reset_proxy_usage(db)
        return {"message": "í”„ë¡ì‹œ ì‚¬ìš© ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        print(f"[ERROR] í”„ë¡ì‹œ ê¸°ë¡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="í”„ë¡ì‹œ ê¸°ë¡ ì´ˆê¸°í™” ì‹¤íŒ¨")

@api_router.post("/debug/parse")
def debug_parse_link(data: dict = Body(...)):
    """1fichier ë§í¬ íŒŒì‹± ë””ë²„ê¹… ì—”ë“œí¬ì¸íŠ¸ (ìƒì„¸ ë¡œê·¸ í¬í•¨)"""
    from core.parser import fichier_parser
    
    url = data.get("url")
    password = data.get("password")
    use_proxy = data.get("use_proxy", True)
    
    if not url:
        raise HTTPException(status_code=400, detail="URLì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    debug_info = {
        "url": url,
        "steps": [],
        "success": False,
        "direct_link": None,
        "error": None
    }
    
    try:
        print(f"[DEBUG] íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘: {url}")
        debug_info["steps"].append(f"íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘: {url}")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ HTTP ìš”ì²­ í…ŒìŠ¤íŠ¸
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        }
        
        try:
            import requests
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            simple_response = requests.get(url, headers=headers, timeout=3, verify=False)
            debug_info["steps"].append(f"ê¸°ë³¸ GET ìš”ì²­: {simple_response.status_code}")
            print(f"[DEBUG] ê¸°ë³¸ GET ìš”ì²­ ì‘ë‹µ: {simple_response.status_code}")
        except Exception as e:
            debug_info["steps"].append(f"ê¸°ë³¸ GET ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
            print(f"[DEBUG] ê¸°ë³¸ GET ìš”ì²­ ì‹¤íŒ¨: {e}")
        
        # 2ë‹¨ê³„: cloudscraperë¡œ POST ìš”ì²­
        payload = {'dl_no_ssl': 'on', 'dlinline': 'on'}
        if password:
            payload['pass'] = password
        
        scraper = cloudscraper.create_scraper()
        
        try:
            import ssl
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            r = scraper.post(url, data=payload, headers=headers, timeout=3, verify=False)
            debug_info["steps"].append(f"cloudscraper POST ìš”ì²­: {r.status_code}")
            print(f"[DEBUG] cloudscraper POST ì‘ë‹µ: {r.status_code}")
            
            if r.status_code in [200, 500]:  # HTTP 500ë„ HTML ì‘ë‹µì´ ìˆìœ¼ë©´ ì²˜ë¦¬
                if r.status_code == 500:
                    debug_info["steps"].append(f"HTTP 500 ì˜¤ë¥˜ì§€ë§Œ HTML ì‘ë‹µ ìˆìŒ, íŒŒì‹± ì‹œë„")
                
                html_length = len(r.text)
                debug_info["steps"].append(f"HTML ì‘ë‹µ ê¸¸ì´: {html_length} ë¬¸ì")
                print(f"[DEBUG] HTML ì‘ë‹µ ê¸¸ì´: {html_length}")
                
                # HTML ë‚´ìš© ì¼ë¶€ ë¡œê·¸
                html_preview = r.text[:1000].replace('\n', ' ').replace('\r', '')
                debug_info["steps"].append(f"HTML ë¯¸ë¦¬ë³´ê¸°: {html_preview}")
                print(f"[DEBUG] HTML ë¯¸ë¦¬ë³´ê¸°: {html_preview}")
                
                # 3ë‹¨ê³„: íŒŒì„œë¡œ ë§í¬ ì¶”ì¶œ ì‹œë„
                direct_link = fichier_parser.parse_download_link(r.text, str(url))
                
                if direct_link:
                    debug_info["success"] = True
                    debug_info["direct_link"] = direct_link
                    debug_info["steps"].append(f"ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                    print(f"[DEBUG] ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                    
                    # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    file_info = fichier_parser.extract_file_info(r.text)
                    debug_info["file_info"] = file_info
                    debug_info["steps"].append(f"íŒŒì¼ ì •ë³´: {file_info}")
                else:
                    debug_info["steps"].append("íŒŒì„œì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                    print(f"[DEBUG] íŒŒì„œì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                    
                    # HTMLì—ì„œ ëª¨ë“  ë§í¬ ì¶”ì¶œí•´ì„œ ë””ë²„ê¹…
                    import lxml.html
                    doc = lxml.html.fromstring(r.text)
                    all_links = [a.get('href') for a in doc.xpath('//a[@href]') if a.get('href')]
                    debug_info["all_links"] = all_links[:20]  # ì²˜ìŒ 20ê°œë§Œ
                    debug_info["steps"].append(f"í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ (ì²˜ìŒ 20ê°œ): {all_links[:20]}")
            else:
                debug_info["steps"].append(f"HTTP ì˜¤ë¥˜: {r.status_code}")
                debug_info["error"] = f"HTTP {r.status_code}"
                
        except Exception as e:
            debug_info["steps"].append(f"cloudscraper ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
            debug_info["error"] = str(e)
            print(f"[DEBUG] cloudscraper ìš”ì²­ ì‹¤íŒ¨: {e}")
        
        return debug_info
        
    except Exception as e:
        print(f"[DEBUG] ì „ì²´ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        debug_info["error"] = str(e)
        debug_info["steps"].append(f"ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return debug_info

@api_router.post("/debug/parse-fixed")
def debug_parse_fixed(data: dict = Body(...)):
    """1fichier ë§í¬ íŒŒì‹± (SSL ë¬¸ì œ í•´ê²°ëœ ë²„ì „)"""
    
    url = data.get("url")
    password = data.get("password")
    
    if not url:
        raise HTTPException(status_code=400, detail="URLì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    result = {
        "url": url,
        "steps": [],
        "success": False,
        "direct_link": None,
        "error": None
    }
    
    try:
        result["steps"].append(f"íŒŒì‹± ì‹œì‘: {url}")
        
        # SSL ê²½ê³  ë¹„í™œì„±í™”
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ê¸°ë³¸ requestsë¡œ ì‹œë„
        import requests
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        }
        
        response = requests.get(url, headers=headers, timeout=3, verify=False)
        result["steps"].append(f"GET ìš”ì²­: {response.status_code}")
        
        if response.status_code in [200, 500]:  # HTTP 500ë„ í—ˆìš©
            if response.status_code == 500:
                result["steps"].append("HTTP 500ì´ì§€ë§Œ HTML ì‘ë‹µ ìˆìŒ, ê³„ì† ì§„í–‰")
            
            # HTML íŒŒì‹± ì‹œë„
            from core.parser import fichier_parser
            
            # 60ì´ˆ ëŒ€ê¸° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” 5ì´ˆë§Œ)
            import time
            result["steps"].append("5ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(5)
            
            # POST ìš”ì²­
            post_data = {'dl_no_ssl': 'on', 'dlinline': 'on'}
            if password:
                post_data['pass'] = password
            
            headers['Referer'] = url
            post_response = requests.post(url, data=post_data, headers=headers, timeout=3, verify=False)
            result["steps"].append(f"POST ìš”ì²­: {post_response.status_code}")
            
            if post_response.status_code in [200, 500]:
                if post_response.status_code == 500:
                    result["steps"].append("POST HTTP 500ì´ì§€ë§Œ HTML ì‘ë‹µ ìˆìŒ, íŒŒì‹± ì‹œë„")
                
                # íŒŒì„œë¡œ ë§í¬ ì¶”ì¶œ
                direct_link = fichier_parser.parse_download_link(post_response.text, str(url))
                
                if direct_link and direct_link != str(url):
                    result["success"] = True
                    result["direct_link"] = direct_link
                    result["steps"].append(f"ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                    
                    # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
                    file_info = fichier_parser.extract_file_info(post_response.text)
                    result["file_info"] = file_info
                    
                else:
                    result["steps"].append("íŒŒì„œì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•´ ëª¨ë“  ë§í¬ í‘œì‹œ
                    all_links = re.findall(r'href=["\']([^"\']+)["\']', post_response.text)
                    external_links = [link for link in all_links if link.startswith('http')][:10]
                    result["all_links"] = external_links
                    result["steps"].append(f"ë°œê²¬ëœ ì™¸ë¶€ ë§í¬: {len(external_links)}ê°œ")
            else:
                result["error"] = f"POST ì‹¤íŒ¨: {post_response.status_code}"
        else:
            result["error"] = f"GET ì‹¤íŒ¨: {response.status_code}"
            
    except Exception as e:
        result["error"] = str(e)
        result["steps"].append(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return result

@api_router.post("/debug/simple-test")
def simple_connection_test():
    """ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    test_results = {}
    
    # 1. ê¸°ë³¸ HTTP ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        import requests
        response = requests.get("https://1fichier.com", timeout=3)
        test_results["basic_http"] = {
            "status": "success",
            "status_code": response.status_code,
            "response_length": len(response.text)
        }
    except Exception as e:
        test_results["basic_http"] = {
            "status": "failed",
            "error": str(e)
        }
    
    # 2. cloudscraper í…ŒìŠ¤íŠ¸
    try:
        scraper = cloudscraper.create_scraper()
        response = scraper.get("https://1fichier.com", timeout=3)
        test_results["cloudscraper"] = {
            "status": "success", 
            "status_code": response.status_code,
            "response_length": len(response.text)
        }
    except Exception as e:
        test_results["cloudscraper"] = {
            "status": "failed",
            "error": str(e)
        }
    
    # 3. í”„ë¡ì‹œ ê°œìˆ˜ í™•ì¸
    try:
        user_proxies = get_user_proxy_list(db)
        proxy_count = len(user_proxies)
        test_results["proxy_count"] = proxy_count
    except Exception as e:
        test_results["proxy_error"] = str(e)
    
    return test_results

# Resume download endpoint
@api_router.post("/resume/{download_id}")
async def resume_download(download_id: int, use_proxy: bool = True, db: Session = Depends(get_db)):
    """ë‹¤ìš´ë¡œë“œ ì¬ì‹œì‘"""
    try:
        req = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
        if not req:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if req.status not in [StatusEnum.stopped, StatusEnum.failed]:
            raise HTTPException(status_code=400, detail="ì •ì§€ ë˜ëŠ” ì‹¤íŒ¨ ìƒíƒœì˜ ë‹¤ìš´ë¡œë“œë§Œ ì¬ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # í”„ë¡ì‹œ ì„¤ì • ì—…ë°ì´íŠ¸ 
        req.use_proxy = use_proxy
        req.status = StatusEnum.pending  # ì¼ë‹¨ pendingìœ¼ë¡œ ì„¤ì •, ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì €ê°€ ì‹¤ì œ ìƒíƒœ ê²°ì •
        req.error = None
        db.commit()
        
        print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì¬ì‹œì‘ ìš”ì²­: ID={download_id}, use_proxy={use_proxy}")
        
        # ë‹¤ìš´ë¡œë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        download_manager.start_download(
            download_id,
            download_with_proxy_rotation,
            download_id,
            db,
            force_reparse=False
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì•Œë¦¼ (ì¦‰ì‹œ)
        notify_status_update(db, download_id)
        print(f"[LOG] â˜… ì¬ê°œ WebSocket ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: ID={download_id}, ìƒíƒœ={req.status}")
        
        # ì•½ê°„ ì§€ì—° í›„ ë‹¤ì‹œ í•œë²ˆ ìƒíƒœ ì•Œë¦¼ (race condition ë°©ì§€)
        import asyncio
        asyncio.create_task(delayed_status_update(db, download_id))
        
        return {"message": "ë‹¤ìš´ë¡œë“œê°€ ì¬ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤", "download_id": download_id}
        
    except Exception as e:
        print(f"[ERROR] ë‹¤ìš´ë¡œë“œ ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì¬ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

async def delayed_status_update(db: Session, download_id: int):
    """ì§€ì—°ëœ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    await asyncio.sleep(0.5)  # 500ms ì§€ì—°
    try:
        # DB ìƒˆë¡œê³ ì¹¨ í›„ ìµœì‹  ìƒíƒœë¡œ ë‹¤ì‹œ ì•Œë¦¼
        req = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
        if req:
            notify_status_update(db, download_id)
            print(f"[LOG] â˜… ì§€ì—° ìƒíƒœ ì•Œë¦¼ ì „ì†¡: ID={download_id}, ìƒíƒœ={req.status}")
    except Exception as e:
        print(f"[LOG] ì§€ì—° ìƒíƒœ ì•Œë¦¼ ì‹¤íŒ¨: {e}")

# Retry download endpoint
@api_router.post("/retry/{download_id}")
async def retry_download(download_id: int, db: Session = Depends(get_db)):
    """ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ (ë§í¬ ì¬íŒŒì‹±)"""
    try:
        req = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
        if not req:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìƒíƒœë¥¼ pendingìœ¼ë¡œ ë³€ê²½ (ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì €ê°€ ì‹¤ì œ ìƒíƒœ ê²°ì •)
        req.status = StatusEnum.pending
        req.error = None
        req.direct_link = None  # ë§í¬ ì¬íŒŒì‹±ì„ ìœ„í•´ ì´ˆê¸°í™”
        db.commit()
        
        print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ ìš”ì²­: ID={download_id} (ë§í¬ ì¬íŒŒì‹±)")
        
        # ë‹¤ìš´ë¡œë“œ ìŠ¤ë ˆë“œ ì‹œì‘ (ê°•ì œ ì¬íŒŒì‹±)
        download_manager.start_download(
            download_id,
            download_with_proxy_rotation,
            download_id,
            db,
            force_reparse=True
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì•Œë¦¼ (ì¦‰ì‹œ)
        notify_status_update(db, download_id)
        
        # ì•½ê°„ ì§€ì—° í›„ ë‹¤ì‹œ í•œë²ˆ ìƒíƒœ ì•Œë¦¼ (race condition ë°©ì§€)
        import asyncio
        asyncio.create_task(delayed_status_update(db, download_id))
        
        return {"message": "ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤", "download_id": download_id}
        
    except Exception as e:
        print(f"[ERROR] ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„ ì‹¤íŒ¨: {str(e)}")

# Pause download endpoint
@api_router.post("/pause/{download_id}")
async def pause_download(download_id: int, db: Session = Depends(get_db)):
    """ë‹¤ìš´ë¡œë“œ ì •ì§€"""
    try:
        req = db.query(DownloadRequest).filter(DownloadRequest.id == download_id).first()
        if not req:
            raise HTTPException(status_code=404, detail="ë‹¤ìš´ë¡œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        if req.status not in [StatusEnum.downloading, StatusEnum.proxying, StatusEnum.parsing, StatusEnum.pending]:
            raise HTTPException(status_code=400, detail="ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ëŒ€ê¸° ì¤‘ì¸ ë‹¤ìš´ë¡œë“œë§Œ ì •ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # ìƒíƒœë¥¼ stoppedë¡œ ë³€ê²½
        req.status = StatusEnum.stopped
        db.commit()
        
        print(f"[LOG] ë‹¤ìš´ë¡œë“œ ì •ì§€ ìš”ì²­: ID={download_id}")
        
        # ë‹¤ìš´ë¡œë“œ ë§¤ë‹ˆì €ì—ì„œ í•´ë‹¹ ë‹¤ìš´ë¡œë“œ ê°•ì œ ì œê±°
        if download_manager.is_download_active(download_id):
            download_manager.cancel_download(download_id)
            print(f"[LOG] ë‹¤ìš´ë¡œë“œ {download_id} ìŠ¤ë ˆë“œë¥¼ ë§¤ë‹ˆì €ì—ì„œ ì œê±°")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì•Œë¦¼  
        notify_status_update(db, download_id)
        print(f"[LOG] â˜… ì¼ì‹œì •ì§€ WebSocket ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: ID={download_id}, ìƒíƒœ={req.status}")
        
        return {"message": "ë‹¤ìš´ë¡œë“œê°€ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤", "download_id": download_id}
        
    except Exception as e:
        print(f"[ERROR] ë‹¤ìš´ë¡œë“œ ì •ì§€ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìš´ë¡œë“œ ì •ì§€ ì‹¤íŒ¨: {str(e)}")


@api_router.post("/file-info")
async def get_file_info(data: dict = Body(...)):
    """1fichier URLì—ì„œ íŒŒì¼ ì •ë³´ë§Œ ì¶”ì¶œ (ë‹¤ìš´ë¡œë“œ íì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ)"""
    try:
        url = data.get("url", "").strip()
        if not url:
            raise HTTPException(status_code=400, detail="URLì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        # 1fichier URLì¸ì§€ í™•ì¸ (ì •í™•í•œ ë„ë©”ì¸ ì²´í¬)
        if not re.match(r'https?://(?:[^\.]+\.)?1fichier\.com/', url.lower()):
            raise HTTPException(status_code=400, detail="1fichier URLë§Œ ì§€ì›ë©ë‹ˆë‹¤")
        
        # URLì—ì„œ HTML ê°€ì ¸ì˜¤ê¸°
        import requests
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            raise HTTPException(status_code=404, detail="íŒŒì¼ í˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        from core.parser import fichier_parser
        file_info = fichier_parser.extract_file_info(response.text)
        
        if not file_info or not file_info.get('name'):
            raise HTTPException(status_code=404, detail="íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "success": True,
            "file_info": file_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


@api_router.get("/websocket/stats")
async def get_websocket_stats():
    """WebSocket ì—°ê²° í†µê³„ ì¡°íšŒ"""
    return {
        "active_connections": len(manager.active_connections),
        "max_connections": manager.max_connections
    }

@api_router.post("/telegram/test")
async def test_telegram_notification(data: dict = Body(...)):
    """í…”ë ˆê·¸ë¨ ì•Œë¦¼ í…ŒìŠ¤íŠ¸"""
    try:
        bot_token = data.get("bot_token", "").strip()
        chat_id = data.get("chat_id", "").strip()
        
        if not bot_token or not chat_id:
            raise HTTPException(status_code=400, detail="Bot token and chat ID are required")
        
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡
        import requests
        import json
        
        # ë‹¤êµ­ì–´ ì§€ì›
        from core.download_core import get_translations
        config = get_config()
        lang = config.get("language", "ko")  # ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        translations = get_translations(lang)
        
        test_title = translations.get("telegram_test_notification", "í…ŒìŠ¤íŠ¸ ì•Œë¦¼")  
        test_message = translations.get("telegram_test_message", "OC Proxy Downloaderì—ì„œ ì „ì†¡ëœ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.")
        test_telegram_test = translations.get("telegram_test", "í…”ë ˆê·¸ë¨ í…ŒìŠ¤íŠ¸")
        telegram_test_connected =  translations.get("telegram_test_connected", "í…”ë ˆê·¸ë¨ ì—°ê²° ì„±ê³µ")
        detail_status_text = translations.get("detail_status", "ìƒíƒœ")
        requested_time_text = translations.get("telegram_requested_time", "ìš”ì²­ì‹œê°„")
        
        # HTML í˜•ì‹ìœ¼ë¡œ ì˜ˆìœ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì‘ì„±
        import datetime
        current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        message = f"""ğŸš€ <b>OC-Proxy: {test_title}</b>

âœ… <b>{detail_status_text}: {telegram_test_connected}</b>

<code>ğŸ“± {test_message or 'ì•Œ ìˆ˜ ì—†ìŒ'}</code>

â±ï¸ <b>{requested_time_text}</b>
<code>{current_time or 'ì•Œ ìˆ˜ ì—†ìŒ'}</code>"""
        
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": chat_id,
            "text": message,
            "parse_mode": "HTML"
        }
        
        response = requests.post(url, json=payload, timeout=10)
        
        if response.status_code == 200:
            return {"success": True, "message": "Test notification sent successfully"}
        else:
            error_data = response.json() if response.headers.get('content-type', '').startswith('application/json') else {"description": response.text}
            raise HTTPException(
                status_code=400, 
                detail=f"Telegram API error: {error_data.get('description', 'Unknown error')}"
            )
            
    except requests.RequestException as e:
        print(f"[ERROR] Telegram API request failed: {e}")
        raise HTTPException(status_code=500, detail="Failed to connect to Telegram API")
    except Exception as e:
        print(f"[ERROR] Telegram test failed: {e}")
        raise HTTPException(status_code=500, detail=f"Telegram test failed: {str(e)}")

app.include_router(api_router)
app.include_router(proxy_stats_router, prefix="/api")

# í”„ë¡ì‹œ ê´€ë¦¬ ë¼ìš°í„° ì¶”ê°€
from core.proxy_manager import router as proxy_manager_router
app.include_router(proxy_manager_router, prefix="/api")

# Catch-all route for SPA routing (ì •ì  íŒŒì¼ë³´ë‹¤ ë¨¼ì € ì •ì˜)
@app.get("/{full_path:path}")
async def serve_spa(full_path: str):
    # ì •ì  íŒŒì¼ì¸ì§€ í™•ì¸
    file_path = os.path.join(frontend_dist_path, full_path)
    if os.path.isfile(file_path):
        return FileResponse(file_path)
    # SPA ë¼ìš°íŒ…ì„ ìœ„í•´ index.html ë°˜í™˜
    return FileResponse(os.path.join(frontend_dist_path, "index.html"))

# Serve static files from the dist directory
app.mount("/static", StaticFiles(directory=frontend_dist_path), name="static")

if __name__ == "__main__":
    multiprocessing.freeze_support()
    import uvicorn
    
    # í¬íŠ¸ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
    port = int(os.getenv('UVICORN_PORT', '8000'))
    host = os.getenv('UVICORN_HOST', '0.0.0.0')
    
    print(f"Server started on http://{host}:{port}")
    
    uvicorn.run("main:app", host=host, port=port, reload=False)
