"""
1fichier íŒŒì‹± ì„œë¹„ìŠ¤ ëª¨ë“ˆ
- Direct Link íŒŒì‹±
- ë§í¬ ë§Œë£Œ í™•ì¸
- íŒŒì‹± ë¡œì§ ê´€ë¦¬
"""

import os
import requests
import cloudscraper
import time
from .parser import fichier_parser


def get_or_parse_direct_link(req, proxies=None, use_proxy=False, force_reparse=False, proxy_addr=None):
    """ë‹¤ìš´ë¡œë“œ ìš”ì²­ì—ì„œ ì§ì ‘ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
    
    # proxy_addrì´ ìˆìœ¼ë©´ proxies ìƒì„± (CONNECT í„°ë„ë§ ì‚¬ìš©)
    if proxy_addr and use_proxy:
        proxies = {
            'http': f'http://{proxy_addr}',
            'https': f'http://{proxy_addr}'
        }
        # print(f"[LOG] í”„ë¡ì‹œ ì„¤ì •: {proxy_addr}")
    
    # ê°•ì œ ì¬íŒŒì‹±ì´ ìš”ì²­ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ ë§í¬ê°€ ì—†ëŠ” ê²½ìš°
    if force_reparse or not req.direct_link:
        print(f"[LOG] direct_link ìƒˆë¡œ íŒŒì‹± (force_reparse: {force_reparse}, proxy: {proxy_addr})")
        return parse_direct_link_simple(req.url, req.password, proxies=proxies, use_proxy=use_proxy, proxy_addr=proxy_addr)
    
    # ê¸°ì¡´ ë§í¬ê°€ ìˆëŠ” ê²½ìš° ë§Œë£Œ ì—¬ë¶€ í™•ì¸
    if is_direct_link_expired(req.direct_link, use_proxy=use_proxy, proxy_addr=proxy_addr):
        print(f"[LOG] ê¸°ì¡´ direct_linkê°€ ë§Œë£Œë¨. ì¬íŒŒì‹± ì‹œì‘: {req.direct_link} (proxy: {proxy_addr})")
        return parse_direct_link_simple(req.url, req.password, proxies=proxies, use_proxy=use_proxy, proxy_addr=proxy_addr)
    
    print(f"[LOG] ê¸°ì¡´ direct_link ì¬ì‚¬ìš©: {req.direct_link}")
    return req.direct_link


def parse_direct_link_simple(url, password=None, proxies=None, use_proxy=False, proxy_addr=None):
    """ë‹¨ìˆœí™”ëœ 1fichier Direct Link íŒŒì‹±"""
    # print(f"[LOG] Direct Link íŒŒì‹± ì‹œì‘: {url}")
    
    # ë„ì»¤ í™˜ê²½ì„ ìœ„í•œ ê°•í™”ëœ CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        },
        delay=1  # ìš”ì²­ ê°„ ì§€ì—° ì¶”ê°€
    )
    scraper.verify = False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
    
    # SSL ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (hostname ì²´í¬ ë¹„í™œì„±í™”)
    import ssl
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # requests ì„¸ì…˜ì˜ SSL ì„¤ì • ë³€ê²½
    from requests.adapters import HTTPAdapter
    from urllib3.util.ssl_ import create_urllib3_context
    
    class NoSSLVerifyHTTPAdapter(HTTPAdapter):
        def init_poolmanager(self, *args, **kwargs):
            context = create_urllib3_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            kwargs['ssl_context'] = context
            return super().init_poolmanager(*args, **kwargs)
    
    scraper.mount('https://', NoSSLVerifyHTTPAdapter())
    
    # User-Agent ë¡œí…Œì´ì…˜ì„ ìœ„í•œ ëœë¤ ì„ íƒ
    import random
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
    ]
    
    # ë„ì»¤ í™˜ê²½ì„ ìœ„í•œ ë” ì™„ì „í•œ ë¸Œë¼ìš°ì € í—¤ë”
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br, zstd',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
        'Sec-CH-UA': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'Sec-CH-UA-Mobile': '?0',
        'Sec-CH-UA-Platform': '"Windows"'
    }
    
    # í”„ë¡ì‹œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    if use_proxy and proxies:
        # print(f"[LOG] ì§€ì •ëœ í”„ë¡ì‹œë¡œ íŒŒì‹± ì‹œë„: {proxies}")
        try:
            direct_link, html_content = _parse_with_connection(scraper, url, password, headers, proxies, wait_time_limit=90, proxy_addr=proxy_addr)
            return direct_link  # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
        except (requests.exceptions.ConnectTimeout, 
                requests.exceptions.ReadTimeout, 
                requests.exceptions.Timeout) as e:
            print(f"[LOG] íƒ€ì„ì•„ì›ƒ: {e}")
            raise e  # í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ raise
        except requests.exceptions.ProxyError as e:
            error_msg = str(e)
            proxy_display = proxy_addr if proxy_addr else 'Unknown'
            if "Tunnel connection failed: 400 Bad Request" in error_msg:
                print(f"[LOG] í”„ë¡ì‹œ HTTPS í„°ë„ë§ ì‹¤íŒ¨: {proxy_display}")
            elif "Unable to connect to proxy" in error_msg:
                print(f"[LOG] í”„ë¡ì‹œ ì—°ê²° ë¶ˆê°€: {proxy_display}")
            else:
                print(f"[LOG] í”„ë¡ì‹œ ì—°ê²° ì˜¤ë¥˜ ({proxy_display}): {e}")
            raise e  # í”„ë¡ì‹œ ìˆœí™˜ ë¡œì§ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ raise
        except Exception as e:
            print(f"[LOG] íŒŒì‹± ì˜ˆì™¸: {e}")
            raise e
    
    # ë¡œì»¬ ì—°ê²°ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    else:
        print(f"[LOG] ë¡œì»¬ ì—°ê²°ë¡œ íŒŒì‹± ì‹œë„")
        try:
            direct_link, html_content = _parse_with_connection(scraper, url, password, headers, None, wait_time_limit=90)
            return direct_link  # ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€
        except requests.exceptions.SSLError as e:
            print(f"[LOG] SSL ì—ëŸ¬ ë°œìƒ, ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”í•˜ì—¬ ì¬ì‹œë„: {e}")
            # SSL ì—ëŸ¬ì¸ ê²½ìš° ì¸ì¦ì„œ ê²€ì¦ì„ ì™„ì „íˆ ë¹„í™œì„±í™”í•˜ê³  ì¬ì‹œë„
            scraper.verify = False
            import urllib3
            urllib3.disable_warnings()
            try:
                direct_link, html_content = _parse_with_connection(scraper, url, password, headers, None, wait_time_limit=90)
                return direct_link
            except Exception as retry_e:
                print(f"[LOG] SSL ë¹„í™œì„±í™” í›„ì—ë„ ì‹¤íŒ¨: {retry_e}")
                raise retry_e
        except requests.exceptions.ConnectionError as e:
            print(f"[LOG] ì—°ê²° ì—ëŸ¬ ë°œìƒ: {e}")
            raise e
        except Exception as e:
            print(f"[LOG] ë¡œì»¬ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise e


def parse_direct_link_with_file_info(url, password=None, use_proxy=False, proxy_addr=None):
    """íŒŒì¼ ì •ë³´ì™€ í•¨ê»˜ Direct Link íŒŒì‹± - íŒŒì¼ëª…ì„ ìµœëŒ€í•œ ë¹¨ë¦¬ ì¶”ì¶œí•˜ì—¬ ë³´ì¡´"""
    
    print(f"[LOG] íŒŒì¼ ì •ë³´ ìš°ì„  íŒŒì‹± ì‹œì‘: {url}")
    
    # ë„ì»¤ í™˜ê²½ì„ ìœ„í•œ ê°•í™”ëœ CloudScraper ì„¤ì •
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'desktop': True
        },
        delay=1  # ìš”ì²­ ê°„ ì§€ì—° ì¶”ê°€
    )
    scraper.verify = False  # SSL ê²€ì¦ ë¹„í™œì„±í™”
    
    # SSL ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (hostname ì²´í¬ ë¹„í™œì„±í™”)
    import ssl
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # requests ì„¸ì…˜ì˜ SSL ì„¤ì • ë³€ê²½
    from requests.adapters import HTTPAdapter
    from urllib3.util.ssl_ import create_urllib3_context
    
    class NoSSLVerifyHTTPAdapter(HTTPAdapter):
        def init_poolmanager(self, *args, **kwargs):
            context = create_urllib3_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE
            kwargs['ssl_context'] = context
            return super().init_poolmanager(*args, **kwargs)
    
    scraper.mount('https://', NoSSLVerifyHTTPAdapter())
    
    # User-Agent ë¡œí…Œì´ì…˜ì„ ìœ„í•œ ëœë¤ ì„ íƒ
    import random
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
    ]
    
    # ë„ì»¤ í™˜ê²½ì„ ìœ„í•œ ë” ì™„ì „í•œ ë¸Œë¼ìš°ì € í—¤ë”
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br, zstd',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'DNT': '1',
        'Sec-CH-UA': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'Sec-CH-UA-Mobile': '?0',
        'Sec-CH-UA-Platform': '"Windows"'
    }
    
    # í”„ë¡ì‹œ ì„¤ì •
    proxies = None
    if use_proxy and proxy_addr:
        proxies = {
            'http': f'http://{proxy_addr}',
            'https': f'http://{proxy_addr}'
        }
    
    try:
        # STEP 1: ë¨¼ì € í˜ì´ì§€ì— ì ‘ê·¼í•˜ì—¬ íŒŒì¼ ì •ë³´ë¥¼ ìµœëŒ€í•œ ë¹¨ë¦¬ ì¶”ì¶œ
        print(f"[LOG] 1ë‹¨ê³„: íŒŒì¼ëª… ìš°ì„  ì¶”ì¶œì„ ìœ„í•œ ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ")
        try:
            initial_response = scraper.get(url, headers=headers, proxies=proxies, timeout=15)
            if initial_response.status_code == 200:
                print(f"[LOG] ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ ì„±ê³µ - íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹œë„")
                
                # íŒŒì¼ ì •ë³´ë¥¼ ê°€ëŠ¥í•œ í•œ ë¹¨ë¦¬ ì¶”ì¶œ
                early_file_info = fichier_parser.extract_file_info(initial_response.text)
                if early_file_info and early_file_info.get('name'):
                    print(f"[LOG] â˜… íŒŒì¼ëª… ì¡°ê¸° ì¶”ì¶œ ì„±ê³µ: '{early_file_info['name']}'")
                    
                    # URLë¡œ DBì—ì„œ í•´ë‹¹ ë‹¤ìš´ë¡œë“œ ìš”ì²­ì„ ì°¾ì•„ íŒŒì¼ëª… ì¦‰ì‹œ ì €ì¥
                    temp_db = None
                    try:
                        from .db import SessionLocal
                        from .models import DownloadRequest
                        temp_db = SessionLocal()
                        
                        # URLë¡œ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ì°¾ê¸° (ìµœì‹  ìˆœ)
                        download_req = temp_db.query(DownloadRequest).filter(
                            DownloadRequest.url == url
                        ).order_by(DownloadRequest.requested_at.desc()).first()
                        
                        if download_req:
                            updated = False
                            
                            # íŒŒì¼ëª…ê³¼ í¬ê¸°ê°€ ëª¨ë‘ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸ (ë®ì–´ì“°ê¸° ë°©ì§€)
                            has_filename = download_req.file_name and download_req.file_name.strip() != ''
                            has_filesize = download_req.file_size and download_req.file_size.strip() != ''
                            
                            # ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìƒˆë¡œ ì„¤ì •
                            if not has_filename and not has_filesize and early_file_info.get('name'):
                                download_req.file_name = early_file_info['name']
                                updated = True
                                print(f"[LOG] â˜… íŒŒì¼ëª… ìµœì´ˆ ì„¤ì •: '{early_file_info['name']}'")
                                
                                if early_file_info.get('size'):
                                    download_req.file_size = early_file_info['size']
                                    updated = True
                                    print(f"[LOG] â˜… íŒŒì¼í¬ê¸° ìµœì´ˆ ì„¤ì •: '{early_file_info['size']}'")
                            elif has_filename or has_filesize:
                                print(f"[LOG] â˜… íŒŒì¼ ì •ë³´ ì´ë¯¸ ì¡´ì¬ - ë®ì–´ì“°ê¸° ë°©ì§€ (ì´ë¦„: {has_filename}, í¬ê¸°: {has_filesize})")
                            
                            if updated:
                                temp_db.commit()
                                
                                # WebSocketìœ¼ë¡œ íŒŒì¼ëª…ê³¼ í¬ê¸° ì—…ë°ì´íŠ¸ ì „ì†¡
                                try:
                                    from core.shared import status_queue
                                    import json
                                    message = json.dumps({
                                        "type": "filename_update",
                                        "data": {
                                            "id": download_req.id,
                                            "file_name": download_req.file_name,
                                            "file_size": download_req.file_size,
                                            "url": download_req.url,
                                            "status": download_req.status.value if hasattr(download_req.status, 'value') else str(download_req.status)
                                        }
                                    }, ensure_ascii=False)
                                    status_queue.put(message)
                                except Exception as ws_e:
                                    print(f"[LOG] WebSocket íŒŒì¼ëª… ì—…ë°ì´íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {ws_e}")
                        
                    except Exception as db_e:
                        print(f"[LOG] íŒŒì¼ëª… DB ì¡°ê¸° ì €ì¥ ì‹¤íŒ¨: {db_e}")
                    finally:
                        if temp_db:
                            try:
                                temp_db.close()
                            except:
                                pass
                else:
                    print(f"[LOG] ì´ˆê¸° í˜ì´ì§€ì—ì„œ íŒŒì¼ëª…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ")
            else:
                print(f"[LOG] ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {initial_response.status_code}")
                
        except Exception as early_e:
            print(f"[LOG] íŒŒì¼ëª… ì¡°ê¸° ì¶”ì¶œ ì‹¤íŒ¨: {early_e}")
        
        # STEP 2: ì´ì œ ì •ìƒì ì¸ ë‹¤ìš´ë¡œë“œ ë§í¬ íŒŒì‹± ì§„í–‰
        print(f"[LOG] 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ ë§í¬ íŒŒì‹± ì§„í–‰")
        wait_time_limit = 86400 if use_proxy else 86400  # 24ì‹œê°„ (ìµœëŒ€ ëŒ€ê¸°ì‹œê°„)
        direct_link, html_content = _parse_with_connection(scraper, url, password, headers, proxies, wait_time_limit, proxy_addr=proxy_addr)
        
        if direct_link and html_content:
            # Direct Link ìœ íš¨ì„± ì²´í¬
            if is_direct_link_expired(direct_link, use_proxy=use_proxy, proxy_addr=proxy_addr):
                print(f"[LOG] parse_direct_link_with_file_infoì—ì„œ ë§Œë£Œëœ ë§í¬ ê°ì§€: {direct_link}")
                return None, None
                
            # íŒŒì¼ ì •ë³´ ì¶”ì¶œ (ìµœì¢… í™•ì¸ ë° ë³´ì™„)
            file_info = fichier_parser.extract_file_info(html_content)
            
            # ì¡°ê¸° ì¶”ì¶œí•œ ì •ë³´ì™€ ë¹„êµí•˜ì—¬ ë” ì™„ì „í•œ ì •ë³´ ì‚¬ìš©
            if not file_info.get('name') and 'early_file_info' in locals() and early_file_info.get('name'):
                file_info['name'] = early_file_info['name']
                print(f"[LOG] â˜… ì¡°ê¸° ì¶”ì¶œí•œ íŒŒì¼ëª… ë³µì›: '{file_info['name']}'")
            
            return direct_link, file_info
        
        # ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆì§€ë§Œ íŒŒì¼ ì •ë³´ëŠ” ìˆëŠ” ê²½ìš°
        if 'early_file_info' in locals() and early_file_info.get('name'):
            print(f"[LOG] ë‹¤ìš´ë¡œë“œ ë§í¬ ì‹¤íŒ¨, í•˜ì§€ë§Œ íŒŒì¼ëª…ì€ ë³´ì¡´: '{early_file_info['name']}'")
            return None, early_file_info
        
        return None, None
        
    except Exception as e:
        print(f"[LOG] íŒŒì¼ ì •ë³´ì™€ í•¨ê»˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
        raise e


def _parse_with_connection(scraper, url, password, headers, proxies, wait_time_limit=10, proxy_addr=None, retry_count=5):
    """1fichier ì„¸ì…˜ ê¸°ë°˜ ìˆœì°¨ì  íŒŒì‹± - ìµœëŒ€ 5íšŒ ì‹œë„"""
    import re
    from bs4 import BeautifulSoup
    import time
    
    max_attempts = 5
    attempt = 0
    
    print(f"[LOG] 1fichier ì„¸ì…˜ ê¸°ë°˜ íŒŒì‹± ì‹œì‘ (ìµœëŒ€ {max_attempts}íšŒ ì‹œë„)")
    
    while attempt < max_attempts:
        attempt += 1
        print(f"[LOG] === ì‹œë„ {attempt}/{max_attempts} ===")
        
        try:
            # 1ë‹¨ê³„: í˜ì´ì§€ ë¡œë“œ
            print(f"[LOG] 1fichier í˜ì´ì§€ ë¡œë“œ")
            response = scraper.get(url, headers=headers, proxies=proxies, timeout=30)
            
            if response.status_code != 200:
                print(f"[LOG] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                continue
                
            # 2ë‹¨ê³„: ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë§í¬ê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
            direct_link_patterns = [
                r'href="(https://[a-z0-9\-]+\.1fichier\.com/[^"]+)"[^>]*class="[^"]*(?:ok|btn|download)[^"]*"',  # class ì†ì„± í¬í•¨
                r'<a[^>]+href="(https://[a-z0-9\-]+\.1fichier\.com/[^"]+)"[^>]*>.*?(?:Click|Download|download)[^<]*</a>',  # ë‹¤ìš´ë¡œë“œ í…ìŠ¤íŠ¸
                r'href="(https://[a-z0-9\-]+\.1fichier\.com/[^"]+)"[^>]*style="[^"]*(?:border|red)[^"]*"',  # ë¹¨ê°„ í…Œë‘ë¦¬ ìŠ¤íƒ€ì¼
                r'href="(https://[a-z0-9\-]+\.1fichier\.com/c\d+)"',  # ê°„ë‹¨í•œ cìˆ«ì íŒ¨í„´
            ]
            
            direct_link_match = None
            for pattern in direct_link_patterns:
                direct_link_match = re.search(pattern, response.text, re.IGNORECASE | re.DOTALL)
                if direct_link_match:
                    print(f"[LOG] ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ ë§¤ì¹­: {pattern[:50]}...")
                    break
            if direct_link_match:
                direct_link = direct_link_match.group(1)
                print(f"[LOG] âœ… ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                return direct_link, None
                
            # 3ë‹¨ê³„: ëŒ€ê¸°ì‹œê°„ í™•ì¸ ë° ì¶”ì¶œ
            wait_seconds = None
            button_patterns = [
                r'var\s+ct\s*=\s*(\d+)\s*\*\s*(\d+)',                               # JavaScript var ct = 1*60;
                r'var\s+ct\s*=\s*(\d+)',                                             # JavaScript var ct = 60;
                r'ct\s*=\s*(\d+)\s*\*\s*(\d+)',                                      # ct = 1*60;
                r'ct\s*=\s*(\d+)(?![^\n]*ct--)',                                     # ct = 60; (ê°ì†Œ ì½”ë“œê°€ ì•„ë‹Œ ì´ˆê¸°í™”)
            ]
            
            print(f"[LOG] ëŒ€ê¸°ì‹œê°„ íŒ¨í„´ ê²€ì‚¬ ì¤‘...")
            
            # HTML ë‚´ìš©ì—ì„œ ct ê´€ë ¨ ë¶€ë¶„ ì¶”ì¶œí•´ì„œ ë””ë²„ê¹…
            ct_context_matches = re.findall(r'.{0,50}ct.{0,50}', response.text, re.IGNORECASE)
            if ct_context_matches:
                print(f"[DEBUG] HTMLì—ì„œ 'ct' í¬í•¨ ë¶€ë¶„ë“¤:")
                for i, context in enumerate(ct_context_matches[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                    print(f"[DEBUG]   {i+1}: {context}")
            
            for i, pattern in enumerate(button_patterns):
                matches = re.findall(pattern, response.text, re.IGNORECASE | re.DOTALL)
                if matches:
                    print(f"[DEBUG] íŒ¨í„´ {i+1} ('{pattern}') ëª¨ë“  ë§¤ì¹­: {matches}")
                
                match = re.search(pattern, response.text, re.IGNORECASE | re.DOTALL)
                if match:
                    matched_text = match.group(0)
                    
                    # ê³±ì…ˆ íŒ¨í„´ì¸ì§€ í™•ì¸ (ê·¸ë£¹ì´ 2ê°œì¸ ê²½ìš°)
                    if len(match.groups()) == 2:
                        # ê³±ì…ˆ ê³„ì‚°: ì²« ë²ˆì§¸ * ë‘ ë²ˆì§¸
                        first_num = int(match.group(1))
                        second_num = int(match.group(2))
                        wait_seconds = first_num * second_num
                        print(f"[LOG] ct ê³±ì…ˆ ê³„ì‚°: {first_num} * {second_num} = {wait_seconds}ì´ˆ (ë§¤ì¹­: '{matched_text}')")
                    else:
                        # ë‹¨ì¼ ê°’
                        wait_seconds = int(match.group(1))
                        print(f"[LOG] ct ê°’ ë°œê²¬: {wait_seconds}ì´ˆ (ë§¤ì¹­: '{matched_text}')")
                    
                    # ìœ íš¨í•œ ëŒ€ê¸°ì‹œê°„ì´ ë°œê²¬ë˜ë©´ ë‹¤ë¥¸ íŒ¨í„´ ê²€ì‚¬ ì¤‘ë‹¨
                    break
                    
            if wait_seconds and not (1 <= wait_seconds <= 7200):  # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê²½ìš°ë§Œ
                print(f"[LOG] ëŒ€ê¸°ì‹œê°„ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨: {wait_seconds}ì´ˆ")
                wait_seconds = None
            elif wait_seconds:
                # 1-4ì´ˆì¸ ê²½ìš° ê±°ì˜ ì™„ë£Œëœ ìƒíƒœ - ì§§ê²Œ ëŒ€ê¸° í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ ì¬ê²€ì‚¬
                if wait_seconds <= 4:
                    print(f"[LOG] ë§¤ìš° ì§§ì€ ëŒ€ê¸°ì‹œê°„ ({wait_seconds}ì´ˆ) - ê±°ì˜ ì™„ë£Œëœ ìƒíƒœ")
                        
            # 4ë‹¨ê³„: ëŒ€ê¸°ì‹œê°„ì´ ìˆìœ¼ë©´ ê¸°ë‹¤ë¦¬ê³  POST ìš”ì²­
            if wait_seconds:
                print(f"[LOG] ğŸ• {wait_seconds}ì´ˆ ëŒ€ê¸° ì¤‘... (ì‹œë„ {attempt}/{max_attempts})")
                
                # ëŒ€ê¸° ì‹œì‘í•  ë•Œ ìƒíƒœë¥¼ downloadingìœ¼ë¡œ ì—…ë°ì´íŠ¸
                try:
                    from .download_core import send_websocket_message
                    from .db import SessionLocal
                    from .models import DownloadRequest
                    
                    # DBì—ì„œ ë‹¤ìš´ë¡œë“œ ID ì°¾ê¸°
                    temp_db = SessionLocal()
                    try:
                        download_req = temp_db.query(DownloadRequest).filter(
                            DownloadRequest.url == url
                        ).order_by(DownloadRequest.requested_at.desc()).first()
                        
                        if download_req:
                            # ìƒíƒœë¥¼ downloadingìœ¼ë¡œ ì—…ë°ì´íŠ¸
                            download_req.status = "downloading"
                            temp_db.commit()
                            
                            # WebSocketìœ¼ë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡
                            send_websocket_message("status_update", {
                                "id": download_req.id,
                                "status": "downloading"
                            })
                            print(f"[LOG] ë‹¤ìš´ë¡œë“œ ìƒíƒœë¥¼ 'downloading'ìœ¼ë¡œ ì—…ë°ì´íŠ¸: ID {download_req.id}")
                    finally:
                        temp_db.close()
                except Exception as e:
                    print(f"[LOG] ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                
                # ì‹œê°„ í‘œì‹œ
                if wait_seconds > 300:  # 5ë¶„ ì´ìƒ
                    print(f"[LOG] âš ï¸  ê¸´ ëŒ€ê¸°ì‹œê°„ ê°ì§€: {wait_seconds//60}ë¶„ {wait_seconds%60}ì´ˆ")
                
                # 5ë¶„ ì´ìƒ ëŒ€ê¸°ì‹œê°„ì¼ ë•Œ í…”ë ˆê·¸ë¨ ì•Œë¦¼ (ë¡œì»¬ ë‹¤ìš´ë¡œë“œë§Œ)
                if wait_seconds >= 300:  # 300ì´ˆ = 5ë¶„
                    try:
                        from .download_core import send_telegram_wait_notification
                        # DBì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°
                        from .db import SessionLocal
                        from .models import DownloadRequest
                        
                        with SessionLocal() as db:
                            req = db.query(DownloadRequest).filter(DownloadRequest.url == url).first()
                            file_name = req.file_name if req and req.file_name else "1fichier File"
                        wait_minutes = wait_seconds // 60
                        send_telegram_wait_notification(file_name, wait_minutes, "ko")
                    except Exception as e:
                        print(f"[WARN] í…”ë ˆê·¸ë¨ ëŒ€ê¸°ì‹œê°„ ì•Œë¦¼ ì‹¤íŒ¨: {e}")
                
                # ì‹¤ì œ ëŒ€ê¸° (ëŒ€ê¸°ì‹œê°„ì— ë”°ë¥¸ ìµœì í™”ëœ ì¹´ìš´íŠ¸ë‹¤ìš´)
                if wait_seconds <= 10:
                    # 10ì´ˆ ì´í•˜ ì§§ì€ ëŒ€ê¸°ì‹œê°„ - ê°„ë‹¨íˆ ì²˜ë¦¬
                    print(f"[LOG] ì§§ì€ ëŒ€ê¸°ì‹œê°„ ({wait_seconds}ì´ˆ) - ë‹¨ìˆœ ëŒ€ê¸°")
                    time.sleep(wait_seconds)
                else:
                    # ê¸´ ëŒ€ê¸°ì‹œê°„ - ë¶„ ë‹¨ìœ„ ì¹´ìš´íŠ¸ë‹¤ìš´
                    for remaining in range(wait_seconds, 0, -1):
                        remaining_minutes = remaining // 60
                        remaining_seconds = remaining % 60
                        
                        # WebSocketìœ¼ë¡œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì „ì†¡ (ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸)
                        should_send_update = (
                            remaining <= 10 or  # ë§ˆì§€ë§‰ 10ì´ˆëŠ” ë§¤ì´ˆ
                            remaining % 60 == 0 or  # ë§¤ ë¶„ë§ˆë‹¤
                            (remaining > 300 and remaining % 300 == 0)  # 5ë¶„ ì´ìƒì´ë©´ 5ë¶„ë§ˆë‹¤
                        )
                        
                        if should_send_update:
                            try:
                                from .download_core import send_websocket_message
                                
                                send_websocket_message("wait_countdown", {
                                    "remaining_time": remaining,
                                    "total_wait_time": wait_seconds,
                                    "proxy_addr": proxy_addr,
                                    "url": url
                                })
                            except Exception as e:
                                print(f"[LOG] ì¹´ìš´íŠ¸ë‹¤ìš´ WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")
                        
                        # ë¡œê·¸ ì¶œë ¥ (ë¶„ ë‹¨ìœ„ ì¤‘ì‹¬, 10ì´ˆ ì „ë¶€í„° ìƒì„¸íˆ)
                        if remaining > 10:
                            if remaining % 60 == 0 and remaining_minutes > 0:
                                print(f"[LOG] {remaining_minutes}ë¶„ ëŒ€ê¸°ì¤‘")
                        else:
                            # ë§ˆì§€ë§‰ 10ì´ˆëŠ” ë§¤ì´ˆ í‘œì‹œ
                            print(f"[LOG] ë‚¨ì€ ì‹œê°„: {remaining}ì´ˆ")
                        
                        time.sleep(1)
                
                print(f"[LOG] âœ… ëŒ€ê¸° ì™„ë£Œ! POST ìš”ì²­ ì‹œì‘")
                
                # WebSocketìœ¼ë¡œ ëŒ€ê¸° ì™„ë£Œ ì•Œë¦¼ (ì¹´ìš´íŠ¸ë‹¤ìš´ ì •ë¦¬)
                try:
                    from .download_core import send_websocket_message
                    from .db import SessionLocal
                    from .models import DownloadRequest
                    
                    temp_db = SessionLocal()
                    try:
                        download_req = temp_db.query(DownloadRequest).filter(
                            DownloadRequest.url == url
                        ).order_by(DownloadRequest.requested_at.desc()).first()
                        
                        if download_req:
                            # ëŒ€ê¸° ì™„ë£Œ - wait_info ì •ë¦¬ ë©”ì‹œì§€
                            send_websocket_message("wait_countdown_complete", {
                                "id": download_req.id,
                                "url": url
                            })
                            print(f"[LOG] ëŒ€ê¸° ì™„ë£Œ WebSocket ë©”ì‹œì§€ ì „ì†¡: ID {download_req.id}")
                    finally:
                        temp_db.close()
                except Exception as e:
                    print(f"[LOG] ëŒ€ê¸° ì™„ë£Œ WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")
                
                # 5ë‹¨ê³„: POST ìš”ì²­ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„
                # í¼ ë°ì´í„° ì°¾ê¸°
                form_data = {'submit': 'Download'}
                
                # adz ê°’ ì°¾ê¸°
                adz_match = re.search(r'name="adz"[^>]*value="([^"]*)"', response.text)
                if adz_match:
                    form_data['adz'] = adz_match.group(1)
                
                print(f"[LOG] POST í¼ ë°ì´í„°: {form_data}")
                post_response = scraper.post(url, data=form_data, headers=headers, proxies=proxies, timeout=30)
                
                if post_response.status_code == 200:
                    print(f"[LOG] POST ìš”ì²­ ì„±ê³µ, ë‹¤ìš´ë¡œë“œ ë§í¬ í™•ì¸")
                    response = post_response  # ì‘ë‹µ ì—…ë°ì´íŠ¸
                    
                    # POST í›„ ì‘ë‹µ ë‚´ìš© ê°„ë‹¨íˆ í™•ì¸ (ë””ë²„ê¹…)
                    response_preview = response.text[:500] if len(response.text) > 500 else response.text
                    print(f"[DEBUG] POST ì‘ë‹µ ë‚´ìš© (ì²˜ìŒ 500ì): {response_preview}")
                    
                    # POST í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ ë‹¤ì‹œ í™•ì¸
                    for pattern in direct_link_patterns:
                        direct_link_match = re.search(pattern, response.text, re.IGNORECASE | re.DOTALL)
                        if direct_link_match:
                            direct_link = direct_link_match.group(1)
                            print(f"[LOG] âœ… POST í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ ë°œê²¬: {direct_link}")
                            return direct_link, None
                    
                    print(f"[LOG] POST í›„ì—ë„ ë‹¤ìš´ë¡œë“œ ë§í¬ ì—†ìŒ, ë‹¤ìŒ ì‹œë„ë¡œ ê³„ì†")
                    continue  # ë‹¤ì‹œ ë£¨í”„ ì‹œì‘ (ìƒˆ í˜ì´ì§€ì—ì„œ ë§í¬ ì°¾ê¸°)
                else:
                    print(f"[LOG] POST ìš”ì²­ ì‹¤íŒ¨: {post_response.status_code}")
                    continue
            else:
                print(f"[LOG] âŒ ëŒ€ê¸°ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (ì‹œë„ {attempt})")
                if attempt >= max_attempts:
                    break
                time.sleep(2)  # ì ê¹ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
                
        except Exception as e:
            print(f"[LOG] ì‹œë„ {attempt} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # í”„ë¡ì‹œ ì—°ê²° ì˜¤ë¥˜ì¸ ê²½ìš° ì¬ì‹œë„í•˜ì§€ ì•Šê³  ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬
            error_str = str(e)
            if any(error_keyword in error_str for error_keyword in [
                "Unable to connect to proxy", 
                "WinError 10061", 
                "Connection refused",
                "ProxyError",
                "Failed to establish a new connection"
            ]):
                print(f"[LOG] âŒ í”„ë¡ì‹œ ì—°ê²° ì˜¤ë¥˜ ê°ì§€, ì¬ì‹œë„ ì—†ì´ ë‹¤ìŒ í”„ë¡ì‹œë¡œ ì´ë™")
                break
            
            if attempt >= max_attempts:
                break
            time.sleep(2)
            continue
            
    print(f"[LOG] âŒ {max_attempts}íšŒ ì‹œë„ í›„ ì‹¤íŒ¨")
    return None, None
def _extract_download_link_smart(html_content, original_url):
    """ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì¶”ì¶œ - ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì„œë²„ë§Œ"""
    import re
    from bs4 import BeautifulSoup
    
    try:
        # ëª¨ë“  1fichier.com URL ì°¾ì•„ì„œ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§
        all_urls = re.findall(r'https://[^"\'>\s]*\.1fichier\.com[^"\'>\s]*', html_content)
        
        # ë‹¤ìš´ë¡œë“œ ë§í¬ í›„ë³´ë“¤ì„ ì ìˆ˜í™”
        candidates = []
        for url in set(all_urls):  # ì¤‘ë³µ ì œê±°
            score = 0
            
            # í™•ì‹¤í•œ ì •ì  íŒŒì¼ë“¤ì€ ì œì™¸
            if any(url.lower().endswith(ext) for ext in ['.css', '.js', '.png', '.jpg', '.ico', '.gif']):
                continue
                
            # ì •ì  ì„œë¸Œë„ë©”ì¸ ì œì™¸  
            if any(subdomain in url.lower() for subdomain in ['img.', 'static.', 'www.']):
                continue
                
            # í™•ì‹¤í•œ ë‹¤ìš´ë¡œë“œ ì„œë²„ë“¤
            if re.match(r'https://a-\d+\.1fichier\.com/', url):
                score += 100  # ìµœê³  ì ìˆ˜
            elif re.match(r'https://cdn-\d+\.1fichier\.com/', url):
                score += 90
            elif re.match(r'https://o-\d+\.1fichier\.com/', url):
                score += 80
            elif '/c' in url and re.search(r'/c\d+', url):  # cìˆ«ì íŒ¨í„´
                score += 70
            elif 'dl' in url.lower():
                score += 60
            
            # ê¸´ URL ì„ í˜¸ (ë” ë§ì€ ì •ë³´ í¬í•¨)
            score += min(len(url) // 5, 20)
            
            # ìˆ«ìê°€ ë§ì€ URL ì„ í˜¸ (IDë‚˜ í•´ì‹œ ê°™ìŒ)
            digit_count = len(re.findall(r'\d', url))
            score += digit_count * 2
            
            # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            if '?' in url:
                score += 10
                
            # ì¼ë°˜ì ì¸ í˜ì´ì§€ë“¤ì€ ê°ì 
            if any(page in url.lower() for page in ['login', 'register', 'help', 'cgu', 'tarif']):
                score -= 50
                
            if score > 0:
                candidates.append((score, url))
        
        if candidates:
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ URL ì„ íƒ
            best_score, best_url = max(candidates, key=lambda x: x[0])
            print(f"[LOG] ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ìœ¼ë¡œ ì„ íƒëœ ë§í¬: {best_url} (ì ìˆ˜: {best_score})")
            return best_url
        
        # ë¦¬ë‹¤ì´ë ‰íŠ¸ íŒ¨í„´ë„ ì²´í¬ (Location í—¤ë”ë‚˜ JavaScript)
        redirect_patterns = [
            r'location\.href\s*=\s*["\']([^"\']+)["\']',
            r'window\.location\s*=\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in redirect_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if re.match(r'https://(?:a-\d+|cdn-\d+)\.1fichier\.com/', match):
                    print(f"[LOG] JavaScript ë¦¬ë‹¤ì´ë ‰íŠ¸ ë‹¤ìš´ë¡œë“œ ë§í¬: {match}")
                    return match
        
        print(f"[LOG] ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
        
    except Exception as e:
        print(f"[LOG] ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def parse_file_info_only(url, password=None, use_proxy=True):
    """íŒŒì¼ëª…ê³¼ í¬ê¸°ë§Œ ë¹ ë¥´ê²Œ íŒŒì‹± (ë‹¤ìš´ë¡œë“œ ë§í¬ëŠ” ì œì™¸)"""
    try:
        import cloudscraper
        scraper = cloudscraper.create_scraper()
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        # 1ë‹¨ê³„: í˜ì´ì§€ ë¡œë“œí•˜ì—¬ íŒŒì¼ ì •ë³´ë§Œ ì¶”ì¶œ
        response = scraper.get(url, headers=headers, timeout=20)
        if response.status_code != 200:
            return None
            
        # íŒŒì¼ ì •ë³´ ì¶”ì¶œ
        from .parser import FichierParser
        fichier_parser = FichierParser()
        file_info = fichier_parser.extract_file_info(response.text)
        
        if file_info and file_info.get('name'):
            print(f"[LOG] íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {file_info['name']} ({file_info.get('size', 'ì•Œ ìˆ˜ ì—†ìŒ')})")
            return file_info
        else:
            print(f"[LOG] íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        print(f"[LOG] íŒŒì¼ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None


def _detect_download_limits(html_content, original_url):
    """1fichier ë‹¤ìš´ë¡œë“œ ì œí•œ ìƒí™© ê°ì§€"""
    try:
        import re
        
        # HTML ë‚´ìš© ë””ë²„ê¹…
        print(f"[DEBUG] HTML ê¸¸ì´: {len(html_content)} ê¸€ì")
        
        # HTML ì „ì²´ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ë” ìƒì„¸í•œ ë””ë²„ê¹…
        if len(html_content) > 500:
            # HTML ì „ì²´ë¥¼ ì—¬ëŸ¬ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
            total_len = len(html_content)
            chunk_size = 1000
            
            print(f"[DEBUG] ===== HTML ì „ì²´ ë¶„ì„ (ì´ {total_len}ì) =====")
            
            # ì²« ë²ˆì§¸ 1000ì (ì•ˆì „í•œ ì¶œë ¥)
            try:
                print(f"[DEBUG] HTML ì²« 1000ì:")
                safe_content = html_content[:chunk_size].encode('utf-8', 'ignore').decode('utf-8')
                print(safe_content)
            except:
                print("[DEBUG] HTML ì²« ë¶€ë¶„ ì¶œë ¥ ì‹¤íŒ¨ (ì¸ì½”ë”© ë¬¸ì œ)")
            
            # ì¤‘ê°„ 1000ì  
            try:
                middle_start = total_len // 2 - chunk_size // 2
                middle_end = middle_start + chunk_size
                print(f"[DEBUG] HTML ì¤‘ê°„ 1000ì ({middle_start}-{middle_end}):")
                safe_content = html_content[middle_start:middle_end].encode('utf-8', 'ignore').decode('utf-8')
                print(safe_content)
            except:
                print("[DEBUG] HTML ì¤‘ê°„ ë¶€ë¶„ ì¶œë ¥ ì‹¤íŒ¨ (ì¸ì½”ë”© ë¬¸ì œ)")
            
            # ë§ˆì§€ë§‰ 1000ì
            try:
                print(f"[DEBUG] HTML ë§ˆì§€ë§‰ 1000ì:")
                safe_content = html_content[-chunk_size:].encode('utf-8', 'ignore').decode('utf-8')
                print(safe_content)
            except:
                print("[DEBUG] HTML ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶œë ¥ ì‹¤íŒ¨ (ì¸ì½”ë”© ë¬¸ì œ)")
            
            # íŠ¹ë³„í•œ ìš”ì†Œë“¤ ê²€ì‚¬
            script_matches = re.findall(r'<script[^>]*>(.*?)</script>', html_content, re.IGNORECASE | re.DOTALL)
            if script_matches:
                print(f"[DEBUG] ë°œê²¬ëœ script íƒœê·¸ ìˆ˜: {len(script_matches)}")
                for i, script in enumerate(script_matches[:3]):  # ì²˜ìŒ 3ê°œ ìŠ¤í¬ë¦½íŠ¸ë§Œ
                    print(f"[DEBUG] Script {i+1}: {script[:500]}...")
            
            # form, input, button ë“± ì¤‘ìš” ìš”ì†Œë“¤ ê²€ì‚¬
            forms = re.findall(r'<form[^>]*>.*?</form>', html_content, re.IGNORECASE | re.DOTALL)
            buttons = re.findall(r'<(?:button|input)[^>]*(?:button|submit)[^>]*>', html_content, re.IGNORECASE)
            
            print(f"[DEBUG] ë°œê²¬ëœ form ìˆ˜: {len(forms)}")
            print(f"[DEBUG] ë°œê²¬ëœ button/input ìˆ˜: {len(buttons)}")
            
            if buttons:
                print(f"[DEBUG] ë²„íŠ¼ë“¤: {buttons}")
        
        if 'dlw' in html_content:
            print(f"[DEBUG] HTMLì—ì„œ 'dlw' ë°œê²¬ë¨")
        if 'disabled' in html_content:
            print(f"[DEBUG] HTMLì—ì„œ 'disabled' ë°œê²¬ë¨")
        if 'Free download' in html_content:
            print(f"[DEBUG] HTMLì—ì„œ 'Free download' ë°œê²¬ë¨")
        if 'button' in html_content.lower():
            print(f"[DEBUG] HTMLì—ì„œ 'button' ë°œê²¬ë¨")
        if '1fichier' in html_content:
            print(f"[DEBUG] HTMLì—ì„œ '1fichier' ë°œê²¬ë¨")
        else:
            print(f"[DEBUG] HTMLì—ì„œ '1fichier'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²ƒ ê°™ìŒ")
        
        # 1ë‹¨ê³„: JavaScriptì—ì„œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„ ì¶”ì¶œ (ìµœìš°ì„ )
        # ë¨¼ì € JavaScript ì¹´ìš´íŠ¸ë‹¤ìš´ ë³€ìˆ˜ë¥¼ ì°¾ê¸° (dlw ë²„íŠ¼ ìœ ë¬´ì™€ ê´€ê³„ì—†ì´)
        js_countdown_patterns = [
            # 2025ë…„ 1fichier ìµœì‹  íŒ¨í„´ë“¤ ì¶”ê°€
            r'function\s+ctt\s*\(\s*\)\s*\{.*?ct\s*=\s*(\d+)',     # function ctt() { ct = 60; }
            r'var\s+ct\s*=\s*(\d+)',                                # var ct = 60
            r'ct\s*=\s*(\d+)',                                      # ct = 60
            r'let\s+ct\s*=\s*(\d+)',                               # let ct = 60
            r'const\s+ct\s*=\s*(\d+)',                             # const ct = 60
            r'ctt\s*\(\s*\).*?(\d+)',                              # ctt() function reference with number
            r'Free download.*?(\d+).*?second',                      # Free download ... 60 ... seconds
            r'wait.*?(\d+).*?second',                              # wait 60 seconds
            r'(\d+)\s*second.*?download',                          # 60 seconds ... download
            r'var\s+ct\s*=\s*(\d+)\s*\*\s*(\d+)',                 # var ct = 1*60 -> ê³±ì…ˆ ê²°ê³¼ ê³„ì‚°
            r'ct\s*=\s*(\d+)\s*\*\s*(\d+)',                       # ct = 1*60 -> ê³±ì…ˆ ê²°ê³¼ ê³„ì‚°
            r'countdown\s*=\s*(\d+)',                              # countdown = 45
            r'timer\s*=\s*(\d+)',                                 # timer = 30
            r'waitTime\s*=\s*(\d+)',                              # waitTime = 25
            r'delay\s*=\s*(\d+)',                                 # delay = 15
            r'var\s+\w*[tT]ime\w*\s*=\s*(\d+)',                   # var waitTime = 60, var countTime = 45
            r'setTimeout\s*\(\s*\w+\s*,\s*(\d+)\s*\*\s*1000\s*\)', # setTimeout(func, 60 * 1000)
            r'setInterval\s*\(\s*\w+\s*,\s*1000\s*\).*?(\d+)',     # setIntervalê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ìˆ«ì
        ]
        
        for pattern in js_countdown_patterns:
            js_match = re.search(pattern, html_content, re.IGNORECASE)
            if js_match:
                # ê³±ì…ˆ íŒ¨í„´ì¸ ê²½ìš° ê³„ì‚°
                if len(js_match.groups()) >= 2 and js_match.group(2):
                    countdown_seconds = int(js_match.group(1)) * int(js_match.group(2))
                    print(f"[LOG] JavaScript ê³±ì…ˆ íŒ¨í„´ ê°ì§€: {js_match.group(1)} * {js_match.group(2)} = {countdown_seconds}ì´ˆ")
                else:
                    countdown_seconds = int(js_match.group(1))
                    print(f"[LOG] JavaScript ë‹¨ìˆœ íŒ¨í„´ ê°ì§€: {countdown_seconds}ì´ˆ")
                
                # í•©ë¦¬ì ì¸ ë²”ìœ„ ì²´í¬ (5ì´ˆ~3600ì´ˆ = 1ì‹œê°„ê¹Œì§€)
                if 5 <= countdown_seconds <= 7200:
                    print(f"[LOG] JavaScript íŒ¨í„´ì—ì„œ ì¹´ìš´íŠ¸ë‹¤ìš´ ê°ì§€: {countdown_seconds}ì´ˆ (íŒ¨í„´: {pattern})")
                    return ("countdown", countdown_seconds)
        
        # 2ë‹¨ê³„: dlw ë²„íŠ¼ í™•ì¸ (JavaScript ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if 'id="dlw"' in html_content and 'disabled' in html_content:
            print(f"[DEBUG] dlw ë²„íŠ¼ì´ disabled ìƒíƒœë¡œ ë°œê²¬ë¨ (JavaScript ì‹œê°„ ì—†ìŒ)")
        else:
            print(f"[DEBUG] dlw ë²„íŠ¼ì´ë‚˜ disabled ì†ì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # JavaScript ë˜ëŠ” HTMLì— íŠ¹ì • í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        countdown_keywords = ['countdown', 'timer', 'wait', 'delay', 'second', 'sec', 'ì´ˆ']
        found_keywords = [kw for kw in countdown_keywords if kw in html_content.lower()]
        if found_keywords:
            print(f"[DEBUG] ì¹´ìš´íŠ¸ë‹¤ìš´ ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬: {found_keywords}")
        else:
            print(f"[DEBUG] ì¹´ìš´íŠ¸ë‹¤ìš´ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # ê¸°ì¡´ íŒ¨í„´ë“¤ë„ ì‹œë„
        countdown_patterns = [
            r'Free download in.*?(\d+)',      # Free download in 47 or Free download in â³ 47
            r'download in.*?(\d+)',           # download in 25
            r'disabled.*?(\d+)',              # disabled button with countdown
        ]
        
        for pattern in countdown_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                countdown_seconds = int(match.group(1))
                print(f"[LOG] ì¹´ìš´íŠ¸ë‹¤ìš´ íŒ¨í„´ ë§¤ì¹­ë¨: '{pattern}' -> {countdown_seconds}ì´ˆ")
                return ("countdown", countdown_seconds)
        
        # 2ë‹¨ê³„: "You must wait X minutes" í˜•íƒœì˜ ë©”ì‹œì§€
        wait_patterns = [
            (r'You must wait\s+(\d+)\s+minutes?', "ëŒ€ê¸° ì‹œê°„"),
            (r'must wait\s+(\d+)\s+minutes?', "ëŒ€ê¸° ì‹œê°„"), 
            (r'wait\s+(\d+)\s+minutes?', "ëŒ€ê¸° ì‹œê°„"),
            (r'vous devez attendre\s+(\d+)', "ëŒ€ê¸° ì‹œê°„"),
        ]
        
        for pattern, limit_type in wait_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                wait_minutes = int(match.group(1))
                return (limit_type, f"{wait_minutes} ë¶„")
        
        # 3ë‹¨ê³„: HTMLì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸° (ë” ë„“ì€ ë²”ìœ„)
        html_countdown_patterns = [
            # ì‹¤ì œ 1fichier dlw ë²„íŠ¼ íŒ¨í„´ (ìµœìš°ì„ ) - ì œê³µëœ HTML ê¸°ì¤€
            r'Free\s+download\s+in\s+â³\s+(\d+)',                        # Free download in â³ 888 (ì‹¤ì œ íŒ¨í„´)
            r'Free\s+download\s+in\s+[^\d]*(\d+)',                       # Free download in [ì•„ë¬´ê±°ë‚˜] 888
            r'>Free\s+download\s+in\s+.*?(\d+)</button>',                # ë²„íŠ¼ íƒœê·¸ ë‚´ë¶€ì˜ íŒ¨í„´
            r'id=[\'"]dlw[\'"][^>]*>.*?Free\s+download\s+in\s+.*?(\d+)', # dlw ë²„íŠ¼ì˜ ì •í™•í•œ íŒ¨í„´
            
            # ê¸°ì¡´ íŒ¨í„´ë“¤ (ë°±ì—…ìš©)
            r'Free\s+download\s+is\s+available\s+in\s+(\d+)\s+seconds?', # Free download is available in 60 seconds
            r'Please\s+wait\s+(\d+)\s+seconds?',                         # Please wait 60 seconds
            r'Download\s+will\s+be\s+available\s+in\s+(\d+)\s+seconds?', # Download will be available in 60 seconds
            r'Wait\s+(\d+)\s+seconds?',                                   # Wait 60 seconds
            r'Attendez\s+(\d+)\s+secondes?',                             # French: Attendez 60 secondes
            r'TÃ©lÃ©chargement.*?(\d+)\s+secondes?',                       # French: TÃ©lÃ©chargement dans 60 secondes
            r'disabled[^>]*>.*?(\d+).*?second',                          # disabled button with seconds
            r'id=[\'"]dlw[\'"][^>]*>.*?(\d+)',                          # dlw button with number
            r'button[^>]*disabled[^>]*>.*?(\d+)',                        # disabled button with number
            r'(\d+)\s*seconds?',                                         # "60 seconds" í˜•íƒœ
            r'(\d+)\s*sec',                                              # "60 sec" í˜•íƒœ  
            r'wait.*?(\d+)',                                             # "wait 45" í˜•íƒœ
            r'countdown.*?(\d+)',                                        # "countdown 30" í˜•íƒœ
            r'(\d+)\s*(?:ì´ˆ|seconds?|sec)',                              # í•œêµ­ì–´/ì˜ì–´ ì´ˆ í‘œì‹œ
        ]
        
        for pattern in html_countdown_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                countdown_seconds = int(match.group(1))
                # í•©ë¦¬ì ì¸ ì‹œê°„ ë²”ìœ„ì¸ì§€ í™•ì¸ (5ì´ˆ~7200ì´ˆ = 2ì‹œê°„ê¹Œì§€)
                if 5 <= countdown_seconds <= 7200:
                    print(f"[LOG] HTML íŒ¨í„´ì—ì„œ ì¹´ìš´íŠ¸ë‹¤ìš´ ê°ì§€: {countdown_seconds}ì´ˆ")
                    return ("countdown", countdown_seconds)
        
        # 4ë‹¨ê³„: ë” ê´‘ë²”ìœ„í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¹´ìš´íŠ¸ë‹¤ìš´ ê°ì§€
        # HTML í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ í‘œì‹œë¥¼ ì°¾ê¸°
        text_time_patterns = [
            r'(\d+)\s*second',                    # "60 second" 
            r'(\d+)\s*sec',                       # "45 sec"
            r'(\d+)\s*\s*ì´ˆ',                     # "30 ì´ˆ" (í•œêµ­ì–´)
            r'Free\s+download\s+in\s+(\d+)',      # "Free download in 60"
            r'Download\s+in\s+(\d+)',             # "Download in 45"
            r'Please\s+wait\s+(\d+)',             # "Please wait 30"
            r'Wait\s+(\d+)',                      # "Wait 25"
            r'Countdown:\s*(\d+)',                # "Countdown: 20"
        ]
        
        for pattern in text_time_patterns:
            text_match = re.search(pattern, html_content, re.IGNORECASE)
            if text_match:
                countdown_seconds = int(text_match.group(1))
                # í•©ë¦¬ì ì¸ ë²”ìœ„ ì²´í¬ (5ì´ˆ~3600ì´ˆ = 1ì‹œê°„ê¹Œì§€)
                if 5 <= countdown_seconds <= 7200:
                    print(f"[LOG] HTML í…ìŠ¤íŠ¸ì—ì„œ ì¹´ìš´íŠ¸ë‹¤ìš´ ê°ì§€: {countdown_seconds}ì´ˆ (íŒ¨í„´: {pattern})")
                    return ("countdown", countdown_seconds)
        
        # 5ë‹¨ê³„: ìˆ«ì íŒ¨í„´ ê´‘ë²”ìœ„ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ì‹œë„)
        # ëª¨ë“  ìˆ«ìë¥¼ ì°¾ì•„ì„œ ì¹´ìš´íŠ¸ë‹¤ìš´ í›„ë³´ ê²€ì‚¬ (ìµœëŒ€ 4ìë¦¬ê¹Œì§€ - 7200ì´ˆ ì§€ì›)
        all_numbers = re.findall(r'\b(\d{1,4})\b', html_content)
        reasonable_countdown_numbers = [int(n) for n in all_numbers if 10 <= int(n) <= 7200]
        
        if reasonable_countdown_numbers:
            print(f"[DEBUG] HTMLì—ì„œ ë°œê²¬ëœ ì¹´ìš´íŠ¸ë‹¤ìš´ í›„ë³´ ìˆ«ìë“¤: {reasonable_countdown_numbers[:10]}")
            # ê°€ì¥ í”í•œ ìˆ«ìë‚˜ íŠ¹ì • ë²”ìœ„ì˜ ìˆ«ìë¥¼ ì¹´ìš´íŠ¸ë‹¤ìš´ìœ¼ë¡œ ì¶”ì •
            from collections import Counter
            counter = Counter(reasonable_countdown_numbers)
            most_common = counter.most_common(1)
            if most_common:
                candidate_countdown = most_common[0][0]
                print(f"[LOG] ì¶”ì • ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„: {candidate_countdown}ì´ˆ (HTML ë‚´ ë¹ˆë„ ê¸°ë°˜)")
                return ("countdown", candidate_countdown)
        
        # 6ë‹¨ê³„: ë‹¤ë¥¸ ì ‘ê·¼ - URL íŒ¨í„´ ë¶„ì„
        # 1fichier URLì—ì„œ íŠ¹ë³„í•œ íŒ¨í„´ì´ë‚˜ íŒŒë¼ë¯¸í„° í™•ì¸
        if '1fichier.com' in original_url:
            print(f"[DEBUG] 1fichier URL í™•ì¸ë¨. URL íŒ¨í„´ ë¶„ì„...")
            
            # URLì—ì„œ íŠ¹ë³„í•œ ë§¤ê°œë³€ìˆ˜ë‚˜ íŒ¨í„´ í™•ì¸
            if '?download=' in original_url or '&download=' in original_url:
                print(f"[DEBUG] Direct download URL íŒ¨í„´ ê°ì§€")
                return (None, None)  # ì œí•œ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
            
            # ì¼ë°˜ì ì¸ 1fichier íŒŒì¼ URL íŒ¨í„´ì¸ ê²½ìš° ê¸°ë³¸ ëŒ€ê¸°ì‹œê°„ ì ìš©
            if re.match(r'https?://1fichier\.com/\?\w+', original_url):
                # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                if any(indicator in html_content.lower() for indicator in [
                    'file not found', 'fichier introuvable', 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†', 
                    'does not exist', 'n\'existe pas', 'error 404'
                ]):
                    print(f"[LOG] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì‚­ì œë¨")
                    return ("not_found", "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                
                # dlw ë²„íŠ¼ì´ ìˆìœ¼ë©´ ì¹´ìš´íŠ¸ë‹¤ìš´ ì ìš©
                if 'id="dlw"' in html_content or 'dlw' in html_content:
                    print(f"[LOG] í‘œì¤€ 1fichier URL íŒ¨í„´ + dlw ë²„íŠ¼ ì¡´ì¬ - ê¸°ë³¸ ì¹´ìš´íŠ¸ë‹¤ìš´ 60ì´ˆ ì ìš©")
                    return ("countdown", 60)
                else:
                    print(f"[LOG] 1fichier URLì´ì§€ë§Œ dlw ë²„íŠ¼ ì—†ìŒ - ì¶”ê°€ ë¶„ì„ í•„ìš”")
                    return (None, None)
        
        # 7ë‹¨ê³„: í”„ë¦¬ë¯¸ì—„ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš° (ìµœì¢… ì²´í¬)
        premium_indicators = [
            '/console/abo.pl' in html_content,
            'premium required' in html_content.lower(),
            'premium account' in html_content.lower(), 
            'upgrade to premium' in html_content.lower(),
            'subscription' in html_content.lower() and 'payment' in html_content.lower()
        ]
        
        # ë§¤ìš° í™•ì‹¤í•œ í”„ë¦¬ë¯¸ì—„ í˜ì´ì§€ í‘œì‹œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ
        if any(premium_indicators) and 'countdown' not in html_content.lower() and 'timer' not in html_content.lower():
            print(f"[DEBUG] í”„ë¦¬ë¯¸ì—„ í•„ìš” ê°ì§€ë¨ (ì¹´ìš´íŠ¸ë‹¤ìš´ ê´€ë ¨ ë‹¨ì–´ ì—†ìŒ)")
            return ("ë‹¤ìš´ë¡œë“œ ì œí•œ - í”„ë¦¬ë¯¸ì—„ í•„ìš”", None)
        
        # 4ë‹¨ê³„: ê¸°íƒ€ ì‹œê°„ ì œí•œ ë©”ì‹œì§€ë“¤
        time_limit_patterns = [
            (r'(\d+)\s*minutes?\s*before', "ì‹œê°„ ì œí•œ"),
            (r'(\d+)\s*hours?\s*before', "ì‹œê°„ ì œí•œ"),
            (r'please wait\s*(\d+)', "ëŒ€ê¸° ì‹œê°„"),
        ]
        
        for pattern, limit_type in time_limit_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                remaining_time = f"{match.group(1)} ë¶„"
                return (limit_type, remaining_time)
        
        # 5ë‹¨ê³„: ì¼ì¼ ë‹¤ìš´ë¡œë“œ ì œí•œ
        daily_limit_keywords = [
            'daily limit',
            'limite quotidienne',
            'ì¼ì¼ ì œí•œ',
            'download limit reached',
            'limite de tÃ©lÃ©chargement'
        ]
        
        for keyword in daily_limit_keywords:
            if keyword.lower() in html_content.lower():
                return ("ì¼ì¼ ë‹¤ìš´ë¡œë“œ ì œí•œ", None)
        
        # 6ë‹¨ê³„: IP ì œí•œ
        ip_limit_keywords = [
            'ip address',
            'adresse ip',
            'too many connections',
            'trop de connexions'
        ]
        
        for keyword in ip_limit_keywords:
            if keyword.lower() in html_content.lower():
                return ("IP ì œí•œ", None)
        
        # ìµœì¢… fallback: 1fichier ì‚¬ì´íŠ¸ì¸ë° ëª…í™•í•œ ë‹¤ìš´ë¡œë“œ ë§í¬ê°€ ì—†ë‹¤ë©´ ê¸°ë³¸ ëŒ€ê¸°ì‹œê°„ ì ìš©
        if '1fichier.com' in original_url and 'download' not in html_content.lower():
            print(f"[LOG] 1fichier ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê¸°ë³¸ ëŒ€ê¸°ì‹œê°„ 60ì´ˆ ì ìš©")
            return ("countdown", 60)
        
        print(f"[DEBUG] ì–´ë–¤ ì œí•œë„ ê°ì§€ë˜ì§€ ì•ŠìŒ")
        return None
        
    except Exception as e:
        print(f"[LOG] ì œí•œ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def is_direct_link_expired(direct_link, use_proxy=False, proxy_addr=None):
    """direct_linkê°€ ë§Œë£Œë˜ì—ˆëŠ”ì§€ ê°„ë‹¨íˆ ì²´í¬"""
    if not direct_link:
        return True
    
    # í”„ë¡ì‹œ ì„¤ì •
    proxies = None
    if use_proxy and proxy_addr:
        proxies = {
            'http': f'http://{proxy_addr}',
            'https': f'http://{proxy_addr}'
        }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': '*/*',
        'Accept-Language': 'ko-KR,ko;q=0.9',
        'Connection': 'keep-alive',
    }
    
    try:
        # HEAD ìš”ì²­ìœ¼ë¡œ ë§í¬ ìœ íš¨ì„± í™•ì¸ (íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼)
        response = requests.head(direct_link, headers=headers, timeout=(5, 10), allow_redirects=True, proxies=proxies)
        print(f"[LOG] Direct Link ìœ íš¨ì„± ê²€ì‚¬: {response.status_code}")
        
        if response.status_code in [200, 206]:  # 200 OK ë˜ëŠ” 206 Partial Content
            return False
        elif response.status_code in [403, 404, 410, 429]:  # ë§Œë£Œë˜ê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€
            print(f"[LOG] Direct Link ë§Œë£Œ ê°ì§€: {response.status_code}")
            return True
        else:
            print(f"[LOG] ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ: {response.status_code}")
            return True
    except Exception as e:
        error_str = str(e)
        print(f"[LOG] Direct Link ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        
        # DNS ì˜¤ë¥˜ì¸ ê²½ìš° í™•ì‹¤íˆ ë§Œë£Œëœ ê²ƒìœ¼ë¡œ íŒë‹¨
        if any(dns_error in error_str for dns_error in [
            "NameResolutionError", "Failed to resolve", "Name or service not known", 
            "No address associated with hostname", "nodename nor servname provided",
            "dstorage.fr"  # íŠ¹ë³„íˆ dstorage.fr DNS ì˜¤ë¥˜ ê°ì§€
        ]):
            print(f"[LOG] DNS í•´ìƒë„ ì˜¤ë¥˜ë¡œ ì¸í•œ ë§í¬ ë§Œë£Œ í™•ì •: {error_str}")
            
        return True  # ê¸°íƒ€ ì—ëŸ¬ ì‹œ ë§Œë£Œë¡œ ê°„ì£¼